{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data Cleaning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rapidfuzz import process\n",
    "from os.path import exists\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringConvert(x):\n",
    "    return x if type(x) == str else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that makes dictionary that combines 3 full name columns into 1\n",
    "def genFullNameDict(df):    \n",
    "    namemerge_pre = dict()\n",
    "    for x in df.index:\n",
    "        marker = False\n",
    "        row = df.loc[x][['full name 1', 'full name 2', 'full name 3']]\n",
    "        #identify number of unique names\n",
    "        if len(row[row.notna()].unique()) > 1:\n",
    "            row_names = row[row.notna()].unique()\n",
    "            for name in row_names:\n",
    "                if name in namemerge_pre.keys() and not marker:\n",
    "                    #add to dictionary if entry already exists\n",
    "                    namemerge_pre[name].extend([n for n in row_names if \n",
    "                                                n != name and n not in namemerge_pre[name]])\n",
    "                    marker = True\n",
    "            #add to dictionary using shortest name as key\n",
    "            if not marker:\n",
    "                minname = row_names[0]\n",
    "                for name in row_names:\n",
    "                    if len(name) < len(minname):\n",
    "                        minname = name\n",
    "                namemerge_pre[minname] = [n for n in row_names if n != minname]\n",
    "    \n",
    "    #invert the dictionary - swap keys and values\n",
    "    namemerge = dict()\n",
    "    for key in namemerge_pre.keys():\n",
    "        vals = namemerge_pre[key]\n",
    "        #iterate through list of values\n",
    "        for val in vals:\n",
    "            namemerge[val] = key\n",
    "    \n",
    "    return namemerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the fullname 1/2/3 columns to generate full name column\n",
    "def setFullName(df):\n",
    "    for x in df.index:\n",
    "        marker = False\n",
    "        row = df.loc[x][['full name 1', 'full name 2', 'full name 3']]\n",
    "        if len(row[row.notna()].unique()) == 0:\n",
    "            df.loc[x, 'full name'] = np.nan\n",
    "        else:\n",
    "            df.loc[x, 'full name'] = row[row.notna()].unique()[0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genFuzzyDict(df):\n",
    "    namelst = df['full name'].drop_duplicates()\n",
    "    fn_fuzzy_pre = dict()\n",
    "    for name in namelst:\n",
    "        marker = False\n",
    "        if not pd.isnull(name):\n",
    "            #find matches for name\n",
    "            match = process.extract(name, [x for x in namelst if x != name and not \n",
    "                                           pd.isnull(x)], limit = 1, score_cutoff = 95)\n",
    "            if len(match)> 0:\n",
    "                match = match[0]\n",
    "                if match[1]>95:\n",
    "                    #add suitable matches to dictionary\n",
    "                    for nm in [match[0], name]:\n",
    "                        if nm in fn_fuzzy_pre.keys() and not marker:\n",
    "                            fn_fuzzy_pre[nm].extend([n for n in [match[0], name] if \n",
    "                                                     n != nm and n not in fn_fuzzy_pre[nm]])\n",
    "                            marker = True\n",
    "                    if not marker:\n",
    "                        if len(name) < len(match[0]):\n",
    "                            fn_fuzzy_pre[name] = [match[0]]\n",
    "                        else:\n",
    "                            fn_fuzzy_pre[match[0]] = [name]\n",
    "    #invert dictionary\n",
    "    fn_fuzzy = dict()\n",
    "    for key in fn_fuzzy_pre.keys():\n",
    "        vals = fn_fuzzy_pre[key]\n",
    "        for val in vals:\n",
    "            fn_fuzzy[val] = key\n",
    "    \n",
    "    return fn_fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate a string that contains two names into a list of two names\n",
    "def parseNames(x):\n",
    "    #replace words that don't have meaning\n",
    "    x = x.replace(\"and Co\", \"\").replace(\"and co\", \"\").replace(\"and Others\" ,\"\")\n",
    "    x = x.replace(\"and others\", \"\").replace(\"and Son\", \"\").replace(\"and Sons\", \"\")\n",
    "    x = x.replace(\"and Brothers\", \"\").strip()\n",
    "    #string preprocessing\n",
    "    namelst = x.split(\" and \")\n",
    "    namelst = [x.strip() for x in namelst if x.strip() != \"\"]\n",
    "    if len(namelst) > 1:\n",
    "        wd1len = len(namelst[0].split(\" \"))\n",
    "        wd2len = len(namelst[1].split(\" \"))\n",
    "        #add last name\n",
    "        if wd1len == 1 and wd2len != 1:\n",
    "            namelst[0] = namelst[0] + \" \" + namelst[1].split(\" \")[-1]\n",
    "    return namelst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformdf(df, state):\n",
    "    #add full name columns\n",
    "    df['full name 1'] = (df['First Name'].apply(lambda x: stringConvert(x))\n",
    "                         + \" \" + \n",
    "                         df['Last Name'].apply(lambda x: stringConvert(x)))\n",
    "    df['full name 1'] = df['full name 1'].apply(lambda x: x if len(x.strip().split(\" \")) > 1 else np.nan)\n",
    "    df['full name 2'] = (df['First Name.1'].apply(lambda x: stringConvert(x))\n",
    "                         + \" \" + \n",
    "                         df['Last Name.1'].apply(lambda x: stringConvert(x)))\n",
    "    df['full name 2'] = df['full name 2'].apply(lambda x: x if len(x.strip().split(\" \")) > 1 else np.nan)\n",
    "    df['full name 3'] = (df['First Name.2'].apply(lambda x: stringConvert(x))\n",
    "                         + \" \" + \n",
    "                         df['Last Name.2'].apply(lambda x: stringConvert(x)))\n",
    "    df['full name 3'] = df['full name 3'].apply(lambda x: x if len(x.strip().split(\" \")) > 1 else np.nan)\n",
    "    df['state'] = state\n",
    "    #add dicionary to merge different full name columns into one\n",
    "    namemerge = genFullNameDict(df)\n",
    "    for col in ['full name 1', 'full name 2', 'full name 3']:    \n",
    "        df[col] = df[col].apply(lambda x: namemerge[x] \n",
    "                                if x in namemerge.keys() else x)\n",
    "\n",
    "    df['full name'] = np.nan\n",
    "    df = setFullName(df)\n",
    "    \n",
    "    #fuzzy matching for different names in the full name column\n",
    "    fn_fuzzy = genFuzzyDict(df)\n",
    "    df['full name'] = df['full name'].apply(lambda x: x if x not in fn_fuzzy.keys() else fn_fuzzy[x])\n",
    "    \n",
    "    df_agg = df[['full name', 'state', '6p_Dollar', '6p_Cents', \n",
    "                 '6p_def_Dollar', '6p_def_Cents', '3p_Dollar', '3p_Cents']]\n",
    "    \n",
    "    #do some final preprocessing\n",
    "    df_agg = df_agg[df_agg['full name'].apply(lambda x: not pd.isnull(x))]\n",
    "    df_ind = df_agg[df_agg['full name'].apply(lambda x: len(x.strip().split(\" \")) > 2 \n",
    "                                              and \" and \" in x)].index\n",
    "    df_agg.loc[df_ind, 'full name'] = df_agg.loc[df_ind]['full name'].apply(lambda x: parseNames(x))\n",
    "    df_agg['full name'] = df_agg['full name'].apply(lambda x: [x.strip()] if type(x) != list else x)\n",
    "    \n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformonecoldf(df, state):\n",
    "    #do transformdf but for when you only have one full name oclumn\n",
    "    df['full name'] =  df['First Name'] + \" \" + df['Last Name']\n",
    "    df['state'] = state\n",
    "    fn_fuzzy = genFuzzyDict(df)\n",
    "    df['full name'] = df['full name'].apply(lambda x: \n",
    "                                            x if x not in fn_fuzzy.keys() else fn_fuzzy[x])    \n",
    "    df_agg = df[['full name', 'state', '6p_Dollar', '6p_Cents', \n",
    "                 '6p_def_Dollar', '6p_def_Cents', '3p_Dollar', '3p_Cents']]\n",
    "    \n",
    "    df_agg = df_agg[df_agg['full name'].apply(lambda x: not pd.isnull(x))]\n",
    "    df_ind = df_agg[df_agg['full name'].apply(lambda x: len(x.strip().split(\" \")) > 2 \n",
    "                                              and \" and \" in x)].index\n",
    "    df_agg.loc[df_ind, 'full name'] = df_agg.loc[df_ind]['full name'].apply(lambda x: parseNames(x))\n",
    "    df_agg['full name'] = df_agg['full name'].apply(lambda x: [x.strip()] if type(x) != list else x)\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecticut Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "CT_CD = pd.read_excel(\"Data/Post1790/CT/CT_post1790_CD_ledger.xlsx\", \n",
    "                      header = 13, usecols = 'H, I, N, O, X, Y, AD, AE, AN, AO, AT, AU')\n",
    "CT_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents', \n",
    "                 'First Name.1', 'Last Name.1', '6p_def_Dollar', '6p_def_Cents',\n",
    "                  'First Name.2', 'Last Name.2', '3p_Dollar', '3p_Cents']\n",
    "CT_CD_agg = transformdf(CT_CD, 'CT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maryland Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "MD_CD = pd.read_excel(\"Data/Post1790/MD/MD_post1790_CD.xlsx\", \n",
    "                      header = 11, usecols = 'G, H, L, M, U, V, Z, AA, AI, AJ, AN, AO')\n",
    "MD_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents', \n",
    "                 'First Name.1', 'Last Name.1', '6p_def_Dollar', '6p_def_Cents',\n",
    "                 'First Name.2', 'Last Name.2', '3p_Dollar', '3p_Cents']\n",
    "MD_CD_agg = transformdf(MD_CD, 'MD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([CT_CD_agg, MD_CD_agg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# North Carolina Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "NC_CD = pd.read_excel(\"Data/Post1790/NC/T695_R4_NC_CD.xlsx\", \n",
    "                      header = 11, usecols = 'J, K, W, X, Z, AA, AC, AD ')\n",
    "NC_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents', \n",
    "                 '6p_def_Dollar', '6p_def_Cents', '3p_Dollar', '3p_Cents']\n",
    "NC_CD_agg = transformonecoldf(NC_CD, 'NC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([NC_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Hampshire Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "NH_CD = pd.read_excel(\"Data/Post1790/NH/T652_R6_New_Hampshire_CD.xlsx\", \n",
    "                      header = 10, usecols = 'I, J, N, O, P, Q, R, S')\n",
    "NH_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents',  \n",
    "                 '6p_def_Dollar', '6p_def_Cents', '3p_Dollar', '3p_Cents']\n",
    "NH_CD_agg = transformonecoldf(NH_CD, 'NH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([NH_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "NY_CD = pd.read_excel(\"Data/Post1790/NY/NY_1790_CD.xlsx\", \n",
    "                      header = 11, usecols = 'H, I, M, N, X, Y, AC, AD, AM, AN, AR, AS')\n",
    "NY_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents', \n",
    "                 'First Name.1', 'Last Name.1', '6p_def_Dollar', '6p_def_Cents',\n",
    "                 'First Name.2', 'Last Name.2', '3p_Dollar', '3p_Cents']\n",
    "NY_CD_agg = transformdf(NY_CD, 'NY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([NY_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# South Carolina Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "SC_CD = pd.read_excel(\"Data/Post1790/SC/Post_1790_South_Carolina_CD.xlsx\", \n",
    "                      header = 11, usecols = 'D, E, M, N, S, T, AB, AC, AH, AI, AQ, AR')\n",
    "SC_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents', \n",
    "                 'First Name.1', 'Last Name.1', '6p_def_Dollar', '6p_def_Cents',\n",
    "                 'First Name.2', 'Last Name.2', '3p_Dollar', '3p_Cents']\n",
    "SC_CD_agg = transformdf(SC_CD, 'SC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([SC_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pennsylvania Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "PA_CD = pd.read_excel(\"Data/Post1790/PA/PA_post1790_CD.xlsx\", \n",
    "                      header = 11, usecols = 'G, H, L, M, U, V, Z, AA, AI, AJ, AO, AP')\n",
    "PA_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents', \n",
    "                 'First Name.1', 'Last Name.1', '6p_def_Dollar', '6p_def_Cents',\n",
    "                 'First Name.2', 'Last Name.2', '3p_Dollar', '3p_Cents']\n",
    "PA_CD_agg = transformdf(PA_CD, 'PA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([PA_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rhode Island Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "RI_CD = pd.read_excel(\"Data/Post1790/RI/T653_Rhode_Island_CD.xlsx\", \n",
    "                      header = 11, usecols = 'G, H, L, M, U, V, Z, AA, AI, AJ, AN, AO')\n",
    "RI_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents', \n",
    "                 'First Name.1', 'Last Name.1', '6p_def_Dollar', '6p_def_Cents',\n",
    "                'First Name.2', 'Last Name.2', '3p_Dollar', '3p_Cents']\n",
    "RI_CD_agg = transformdf(RI_CD, 'RI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([RI_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virginia Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "VA_CD = pd.read_excel(\"Data/Post1790/VA/VA_CD.xlsx\", \n",
    "                      header = 11, usecols = 'H, I, K, L, U, V, X, Y, AH, AI, AK, AL')\n",
    "VA_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents', \n",
    "                 'First Name.1', 'Last Name.1', '6p_def_Dollar', '6p_def_Cents',\n",
    "                'First Name.2', 'Last Name.2', '3p_Dollar', '3p_Cents']\n",
    "VA_CD_agg = transformdf(VA_CD, 'VA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([VA_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Georgia Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "GA_CD = pd.read_excel(\"Data/Post1790/GA/T694_GA_Loan_Office_CD.xlsx\", \n",
    "                      header = 10, usecols = 'Q, R, Z, AA, AB, AC, AD, AE')\n",
    "GA_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents',  \n",
    "                 '6p_def_Dollar', '6p_def_Cents', '3p_Dollar', '3p_Cents']\n",
    "GA_CD_agg = transformonecoldf(GA_CD, 'GA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([GA_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Jersey Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "#new jersey is handled manually because it only has 3 percent stock\n",
    "NJ_CD = pd.read_excel(\"Data/Post1790/NJ/NJ_3_percent_stock_T698_R1_R2.xlsx\", \n",
    "                      header = 11, usecols = 'D, E, L, M')\n",
    "NJ_CD.columns = ['First Name', 'Last Name', '3p_Dollar', '3p_Cents']\n",
    "NJ_CD['full name'] =  NJ_CD['First Name'] + \" \" + NJ_CD['Last Name']\n",
    "NJ_CD['state'] = 'NJ'\n",
    "fn_fuzzy = genFuzzyDict(NJ_CD)\n",
    "NJ_CD['full name'] = NJ_CD['full name'].apply(lambda x: x if x not in fn_fuzzy.keys() else fn_fuzzy[x])    \n",
    "NJ_CD_agg = NJ_CD[['full name', 'state', '3p_Dollar', '3p_Cents']]\n",
    "\n",
    "NJ_CD_agg = NJ_CD_agg[NJ_CD_agg['full name'].apply(lambda x: not pd.isnull(x))]\n",
    "NJ_ind = NJ_CD_agg[NJ_CD_agg['full name'].apply(lambda x: len(x.strip().split(\" \")) > 2 \n",
    "                                                and \" and \" in x)].index\n",
    "NJ_CD_agg.loc[NJ_ind, 'full name'] = NJ_CD_agg.loc[NJ_ind]['full name'].apply(lambda x: parseNames(x))\n",
    "NJ_CD_agg['full name'] = NJ_CD_agg['full name'].apply(lambda x: [x.strip()] if type(x) != list else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset in final form\n",
    "cumulative_CD = pd.concat([NJ_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn full name column from list into strings\n",
    "fname = cumulative_CD['full name'].apply(pd.Series)\n",
    "fname.columns = ['full name 1', 'full name 2', 'full name 3']\n",
    "cumulative_CD = pd.concat([cumulative_CD, fname], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual data change\n",
    "cumulative_CD.loc[3881, 'full name 2'] = 'Horace Johnson'\n",
    "cumulative_CD.loc[5190, 'full name 2'] = 'Horace Johnson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many unique individuals were issued 6 percent stocks or deferred 6 percent stocks in 1790 and after?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table of number of unique individuals issued 6% stocks (normal or deferred) by state\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "state\n",
       "CT    754\n",
       "GA     55\n",
       "MD    361\n",
       "NC     54\n",
       "NH    169\n",
       "NJ    568\n",
       "NY    915\n",
       "PA    858\n",
       "RI    513\n",
       "SC    281\n",
       "VA    523\n",
       "Name: full name, dtype: int64"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_6 = cumulative_CD[['6p_Dollar', '6p_Cents', \n",
    "                          '6p_def_Dollar', '6p_def_Cents']].dropna(thresh = 1).index\n",
    "print('table of number of unique individuals issued 6% stocks (normal or deferred) by state')\n",
    "cumulative_CD.groupby('state')['full name'].agg(sum).apply(lambda x: len(set(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many of these individuals\n",
    "- were original purchasers of loan office certicates of the same state as the 6 percent stock?\n",
    "- were original purchasers of loan office certicates issued from another state?\n",
    "- were original recipients of liquidated debtcertiâ€‚cates issued by the same-state loan office? other state loan offices?\n",
    "- were original recipients of the Pierce Certicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_merge(lst1, lst2, threshold=85, limit = 100):\n",
    "    delegates = pd.Series([x for x in lst1.unique() if not pd.isnull(x)])\n",
    "    possible =  [x for x in lst2.unique().tolist() if type(x) == str]\n",
    " \n",
    "    #get matches\n",
    "    #process.extract uses a combination of all four fuzzywuzzy scores\n",
    "    matches = delegates.apply(lambda x: \n",
    "                              process.extract(x, possible, limit=limit, score_cutoff = threshold))\n",
    "    \n",
    "    match_df = pd.DataFrame(columns = ['Delegates', 'Loan Matches'])\n",
    "    #make each match a row in the dataframe\n",
    "    for delegate, matchset in zip(delegates, matches):\n",
    "        matchset_thres = [name for name in matchset if name[1] >= threshold]\n",
    "        if len(matchset_thres) == 0:\n",
    "            add_df = pd.DataFrame(data = {'Delegates': [delegate], 'Loan Matches': [\"\"], 'Scores': [0]})\n",
    "            match_df = pd.concat([match_df, add_df])\n",
    "        else:\n",
    "            delegate_lst = [delegate] * len(matchset_thres)\n",
    "            add_df = pd.DataFrame(data = {'Delegates': delegate_lst, \n",
    "                                          'Loan Matches': [x[0] for x in matchset_thres],\n",
    "                                          'Scores': [x[1] for x in matchset_thres]})\n",
    "            match_df = pd.concat([match_df, add_df])\n",
    "\n",
    "    return match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for performing the second step of the match\n",
    "def matchFunction(lst1, lst2, score = 90):\n",
    "    #check if our matches are actually min 2 words each\n",
    "    #make sure our match is because the individual are similar, not because the phrase or one word in the phrase is similar\n",
    "    #lst1 = list(pd.Series(lst1).unique())\n",
    "    #lst2 = list(pd.Series(lst2).unique())\n",
    "    threshold = min(len(lst1), len(lst2))\n",
    "    matches = 0\n",
    "    nomatch = []\n",
    "    i = 0\n",
    "    for wd1 in lst1:\n",
    "        #modifying which words we compare - dont want to compare first in lst1 with last in lst2\n",
    "        for wd2 in lst2:\n",
    "            if wd1 not in nomatch and process.extract(wd1, [wd2])[0][1] > score:\n",
    "                matches+=1\n",
    "                nomatch.append(wd1)\n",
    "        i+=1\n",
    "    return matches >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produceMatches(delegates, debt, delegate_names, debt_names, threshold = 85):\n",
    "    initial = True\n",
    "    join_df = pd.DataFrame()\n",
    "    #run firs step of matching function\n",
    "    for del_name in delegate_names:\n",
    "        for debt_name in debt_names:\n",
    "            if initial:\n",
    "                join_df = fuzzy_merge(delegates[del_name], debt[debt_name], threshold)\n",
    "                initial = False\n",
    "            else:\n",
    "                add_df = fuzzy_merge(delegates[del_name], debt[debt_name], threshold)\n",
    "                join_df = pd.concat([join_df, add_df])\n",
    "    join_df = join_df.drop_duplicates().reset_index(drop = True)\n",
    "    join_df = join_df[join_df['Scores'].apply(lambda x: x != 0)]\n",
    "    join_df = join_df[join_df['Loan Matches'].apply(lambda x: not pd.isnull(x))]    \n",
    "    #run second step of matching function\n",
    "    join_df_p2 = join_df[join_df['Loan Matches'].apply(lambda x: len(list(set(x.replace(\"??\", \"\").strip().split(\" \"))))>=2)]\n",
    "    join_df_p2_final = join_df_p2[[matchFunction(x.split(\" \"), y.split(\" \")) for x, y in zip(join_df_p2['Delegates'], join_df_p2['Loan Matches'])]]\n",
    "    return join_df_p2_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and preprocess loan office data\n",
    "loan_office = pd.read_csv('Data/Pre1790/cleaned/loan_office_certificates_9_states_cleaned.csv', index_col = 0)\n",
    "states = ['NH', 'MA', 'CT', 'NY', 'NJ', 'PA', 'DE', 'MD', 'VA']\n",
    "num_names = [1, 2, 2, 3, 2, None, 2, None, None]\n",
    "state_names = dict(zip(np.arange(1, 10, 1), states))\n",
    "loan_office['State Name'] = loan_office['State'].apply(lambda x: state_names[x])\n",
    "loan_office['Full Name 1'] = (loan_office['First Name 1 '].apply(lambda x: stringConvert(x)) + \" \" + loan_office['Last Name 1 '].apply(lambda x: stringConvert(x)))\n",
    "loan_office['Full Name 2'] = (loan_office['First Name 2'].apply(lambda x: stringConvert(x)) + \" \" + loan_office['Last Name 2'].apply(lambda x: stringConvert(x)))\n",
    "loan_office['Full Name 3'] = (loan_office['First Name 3'].apply(lambda x: stringConvert(x)) + \" \" + loan_office['Last Name 3'].apply(lambda x: stringConvert(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many individuals were original purchasers of loan office certicates of the same state as the 6 percent stock?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match cd debt data with loan office data from the same state\n",
    "def loanOfficeSameState(state):\n",
    "    #filter for 6% stock\n",
    "    state_ind = cumulative_CD[cumulative_CD['state'] == state][['6p_Dollar', '6p_Cents', '6p_def_Dollar', '6p_def_Cents']].dropna(thresh = 1).index\n",
    "    #skip empty dataframes\n",
    "    if len(state_ind) != 0:\n",
    "        #prepare state and loan office data\n",
    "        state_cd = cumulative_CD.loc[state_ind][['full name 1','full name 2','full name 3', 'state']].drop_duplicates()\n",
    "        state_cd.columns = ['cd name 1','cd name 2','cd name 3', 'cd state']\n",
    "        loan_office_state = loan_office[loan_office['State Name'] == state][['Full Name 1', 'Full Name 2', 'Full Name 3', 'State Name']].drop_duplicates()\n",
    "        loan_office_state.columns = ['loan office name 1', 'loan office name 2', 'loan office name 3', 'loan office state']\n",
    "        #match data\n",
    "        matches = produceMatches(state_cd, loan_office_state, delegate_names = ['cd name 1', 'cd name 2', 'cd name 3'], debt_names = ['loan office name 1', 'loan office name 2', 'loan office name 3'], threshold = 95)\n",
    "        matches['state'] = state\n",
    "        return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loanoffice_samestate = pd.DataFrame({}, columns = ['Delegates', 'Loan Matches', 'Scores', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine matches from all the states\n",
    "for state in states:\n",
    "    df_loanoffice_samestate = pd.concat([df_loanoffice_samestate, loanOfficeSameState(state)])\n",
    "df_loanoffice_samestate.columns = ['CD name', 'Loan Office name', 'Scores', 'state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loanoffice_samestate = df_loanoffice_samestate[df_loanoffice_samestate['CD name'].apply(lambda x: len(x.split(\" \")) > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of individuals who were original purchasers of loan office certicates of the same state as the 6 percent stock\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "state\n",
       "CT    316\n",
       "MD    103\n",
       "NH     62\n",
       "NY     28\n",
       "PA    312\n",
       "VA     61\n",
       "Name: CD name, dtype: int64"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summarize results\n",
    "print(\"Number of individuals who were original purchasers of loan office certicates of the same state as the 6 percent stock\")\n",
    "df_loanoffice_samestate.groupby('state')['CD name'].apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loanoffice_samestate.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual edits\n",
    "rem_ind = [233, 432, 491, 522, 542, 543, 609, 627, 656, 662, 740, 744, 774]\n",
    "df_loanoffice_samestate = df_loanoffice_samestate.loc[[ind for ind in df_loanoffice_samestate.index if ind not in rem_ind]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many individuals were original purchasers of loan office certicates issued from another state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "#higher match threshold for non-same state loan office certificates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match cd debt data with loan office data from a different state\n",
    "def loanOfficeDifState(state):\n",
    "    #filter for 6% stock\n",
    "    state_ind = cumulative_CD[cumulative_CD['state'] == state][['6p_Dollar', '6p_Cents', '6p_def_Dollar', '6p_def_Cents']].dropna(thresh = 1).index\n",
    "    if len(state_ind) != 0:\n",
    "        state_cd = cumulative_CD.loc[state_ind][['full name 1','full name 2','full name 3', 'state']].drop_duplicates()\n",
    "        state_cd.columns = ['cd name 1','cd name 2','cd name 3', 'cd state']\n",
    "        loan_office_nostate = loan_office[loan_office['State Name'] != state][['Full Name 1', 'Full Name 2', 'Full Name 3', 'State Name']].drop_duplicates()\n",
    "        loan_office_nostate.columns = ['loan office name 1', 'loan office name 2', 'loan office name 3', 'loan office state']\n",
    "        #match data\n",
    "        matches = produceMatches(state_cd, loan_office_nostate, delegate_names = ['cd name 1'], debt_names = ['loan office name 1', 'loan office name 2', 'loan office name 3'], threshold = 95)\n",
    "        matches['state'] = state\n",
    "        return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loanoffice_difstate = pd.DataFrame({}, columns = ['Delegates', 'Loan Matches', 'Scores', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine matches from all the states\n",
    "for state in states:\n",
    "    df_loanoffice_difstate = pd.concat([df_loanoffice_difstate, loanOfficeDifState(state)])\n",
    "df_loanoffice_difstate.columns = ['CD name', 'Loan Office name', 'Scores', 'state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loanoffice_difstate = df_loanoffice_difstate[df_loanoffice_difstate['CD name'].apply(lambda x: len(x.split(\" \")) > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "CT    111\n",
       "MD     59\n",
       "NH     41\n",
       "NY     25\n",
       "PA    117\n",
       "VA     98\n",
       "Name: CD name, dtype: int64"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loanoffice_difstate.groupby('state')['CD name'].apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loanoffice_difstate.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual edits\n",
    "rem_ind = [87, 88, 89, 90, 91, 143, 144, 145, 160, 161, 204, 205, 206, 361, 362, 368, 369, 370, 371, 372, 378, 379, 380, 404, 459, 460, 461]\n",
    "df_loanoffice_difstate = df_loanoffice_difstate.loc[[ind for ind in df_loanoffice_difstate.index if ind not in rem_ind]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many individuals were original recipients of liquidated debt certificates issued by the same-state loan office? other state loan offices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match CD state data with liquidated debt from the same state\n",
    "def liquidatedSameStateDebt(state, file, num_names):\n",
    "    #filter for 6% stock   \n",
    "    state_ind = cumulative_CD[cumulative_CD['state'] == state][['6p_Dollar', '6p_Cents']].dropna(thresh = 1).index\n",
    "    if len(state_ind) != 0:\n",
    "        state_cd = cumulative_CD.loc[state_ind][['full name 1','full name 2','full name 3', 'state']].drop_duplicates()\n",
    "        state_cd.columns = ['cd name 1','cd name 2','cd name 3', 'cd state']\n",
    "        #import liquidated state debt files\n",
    "        datafile = 'Data/Pre1790/cleaned/'+file\n",
    "        if exists(datafile):\n",
    "            state_cert = pd.read_csv(datafile, index_col = 0)\n",
    "            namelst = []\n",
    "            #figure out how many full name columns there are in the state liquidated debt file\n",
    "            state_cert['Full Name'] = state_cert['First name'] + \" \" + state_cert['Last name'] \n",
    "            namelst.append('Full Name')\n",
    "            if num_names > 1:\n",
    "                for i in np.arange(2, num_names+1, 1):\n",
    "                    fullname_str = 'Full Name ' + str(i)\n",
    "                    state_cert[fullname_str] = state_cert['First name ' + str(i)] + \" \" + state_cert['Last name ' + str(i)] \n",
    "                    namelst.append(fullname_str)\n",
    "            state_cert_names = state_cert[namelst].drop_duplicates()\n",
    "            #produce matches\n",
    "            matches = produceMatches(state_cd, state_cert_names, delegate_names = ['cd name 1'], debt_names = namelst)\n",
    "            matches['state'] = state\n",
    "            return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samestateliquid = pd.DataFrame({}, columns = ['Delegates', 'Loan Matches', 'Scores', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_name = dict(zip(states, num_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine matches from all the states\n",
    "for state, num_name in state_name.items():\n",
    "    if state != \"PA\":\n",
    "        file = 'liquidated_debt_certificates_'+state+'_cleaned.csv'\n",
    "        df_samestateliquid = pd.concat([df_samestateliquid, liquidatedSameStateDebt(state, file, num_name)])\n",
    "    else:\n",
    "        file1 = 'liquidated_debt_certificates_PA_story_cleaned.csv'\n",
    "        df1 = liquidatedSameStateDebt('PA', file1, 1)\n",
    "        file2 = 'liquidated_debt_certificates_PA_stelle_cleaned.csv'\n",
    "        df2 = liquidatedSameStateDebt('PA', file2, 2)\n",
    "        df = pd.concat([df1, df2]).drop_duplicates()\n",
    "        df_samestateliquid = pd.concat([df_samestateliquid, df])\n",
    "df_samestateliquid.columns = ['CD name', 'Loan Office name', 'Scores', 'state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samestateliquid = df_samestateliquid[df_samestateliquid['CD name'].apply(lambda x: len(x.split(\" \")) > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "CT    100\n",
       "NH     28\n",
       "NY     38\n",
       "PA    213\n",
       "Name: CD name, dtype: int64"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summarize results\n",
    "df_samestateliquid.groupby('state')['CD name'].apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samestateliquid.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual edits\n",
    "rem_ind = [149, 150, 151, 176, 187, 259, 275, 291, 320, 323, 324, 364]\n",
    "df_samestateliquid = df_samestateliquid.loc[[ind for ind in df_samestateliquid.index if ind not in rem_ind]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match CD state data with liquidated debt from a dif state\n",
    "def liquidatedDifStateDebt(state):\n",
    "    #filter for 6% stock   \n",
    "    state_ind = cumulative_CD[cumulative_CD['state'] == state][['6p_Dollar', '6p_Cents']].dropna(thresh = 1).index\n",
    "    if len(state_ind) != 0:\n",
    "        state_cd = cumulative_CD.loc[state_ind][['full name 1','full name 2','full name 3', 'state']].drop_duplicates()\n",
    "        state_cd.columns = ['cd name 1','cd name 2','cd name 3', 'cd state']\n",
    "        #import liquidated state debt files\n",
    "        match_df = pd.DataFrame({}, columns = ['Delegates', 'Loan Matches', 'Scores'])\n",
    "        for statename, num_names in state_name.items():\n",
    "            if not pd.isnull(num_names):\n",
    "                datafile = 'Data/Pre1790/cleaned/liquidated_debt_certificates_'+statename+'_cleaned.csv'\n",
    "                matches = produceLiquidatedMatches(datafile, num_names, state_cd)\n",
    "                match_df = pd.concat([match_df, matches])\n",
    "            elif state == 'PA':    \n",
    "                datafile1 = 'Data/Pre1790/cleaned/liquidated_debt_certificates_PA_story_cleaned.csv'\n",
    "                matches1 = produceLiquidatedMatches(datafile, 1, state_cd)\n",
    "                datafile2 = 'Data/Pre1790/cleaned/liquidated_debt_certificates_PA_stelle_cleaned.csv'\n",
    "                matches2 = produceLiquidatedMatches(datafile, 2, state_cd)\n",
    "                match_df = pd.concat([match_df, matches2])\n",
    "        match_df['state'] = state\n",
    "        return match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_difstateliquid = pd.DataFrame({}, columns = ['Delegates', 'Loan Matches', 'Scores', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine matches from all the states\n",
    "for state in states:\n",
    "    df_difstateliquid = pd.concat([df_difstateliquid, liquidatedDifStateDebt(state)])\n",
    "df_difstateliquid.columns = ['CD name', 'Loan Office name', 'Scores', 'state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_difstateliquid.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual edits\n",
    "rem_ind = [760, 778, 783, 806, 878]\n",
    "df_difstateliquid = df_difstateliquid.loc[[ind for ind in df_difstateliquid.index if ind not in rem_ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "CT    180\n",
       "MD     41\n",
       "NH     59\n",
       "NY     49\n",
       "PA    131\n",
       "VA     57\n",
       "Name: CD name, dtype: int64"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_difstateliquid.groupby('state')['CD name'].apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many individuals were original recipients of the Pierce Certicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "pierce = pd.read_csv('Data/Pre1790/cleaned/'+\"Pierce_Certs_cleaned_2021.csv\", index_col = 0)\n",
    "pierce['Full Name'] = pierce['First'].apply(lambda x: stringConvert(x)) + \" \" + pierce['Last'].apply(lambda x: stringConvert(x))\n",
    "pierce['Full Name 2'] = pierce['First 2'].apply(lambda x: stringConvert(x)) + \" \" + pierce['Last 2'].apply(lambda x: stringConvert(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match cd debt data with loan office data from the Pierce certificate data\n",
    "def pierceCertificates(state):\n",
    "    #filter for 6% stock\n",
    "    state_ind = cumulative_CD[cumulative_CD['state'] == state][['6p_Dollar', '6p_Cents', '6p_def_Dollar', '6p_def_Cents']].dropna(thresh = 1).index\n",
    "    if len(state_ind) != 0:\n",
    "        state_cd = cumulative_CD.loc[state_ind][['full name 1','full name 2','full name 3', 'state']].drop_duplicates()\n",
    "        state_cd.columns = ['cd name 1','cd name 2','cd name 3', 'cd state']\n",
    "        #match data\n",
    "        pierce_names = pierce[pierce['State'].apply(lambda x: pd.isnull(x) or x == state)][['Full Name', 'Full Name 2']].drop_duplicates()\n",
    "        matches = produceMatches(state_cd, pierce_names, delegate_names = ['cd name 1'], debt_names = ['Full Name', 'Full Name 2'])\n",
    "        matches['state'] = state\n",
    "        return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pierce = pd.DataFrame({}, columns = ['Delegates', 'Loan Matches', 'Scores', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in states:\n",
    "    df_pierce = pd.concat([df_pierce, pierceCertificates(state)])\n",
    "df_pierce.columns = ['CD name', 'Loan Office name', 'Scores', 'state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pierce = df_pierce[df_pierce['CD name'].apply(lambda x: len(x.split(\" \")) > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "CT    177\n",
       "MD     84\n",
       "NH     38\n",
       "NY     29\n",
       "PA    171\n",
       "VA    143\n",
       "Name: CD name, dtype: int64"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pierce.groupby('state')['CD name'].apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pierce.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual edits\n",
    "rem_ind = [3, 5, 35, 39, 40, 104, 138, 139, 145, 146, 206, 207, 208, 209, 210, 276, 278, 279, 280, 281, 282, 283, 286, 287, 288, 289, \n",
    "           293, 303, 304, 305, 306, 307, 318, 321, 322, 339, 340, 341, 342, 361, 362, 376, 377, 381, 389, 390, 397, 398, 399, 414, 415, \n",
    "           416, 417, 418, 419, 423, 424, 425, 426, 427, 428, 429, 436, 436, 445, 446, 543, 584, 585, 586, 587, 600, 634, 635, 636, 637, \n",
    "           641, 652, 653, 654, 655, 656, 657, 658, 670, 671]\n",
    "df_pierce = df_pierce.loc[[ind for ind in df_pierce.index if ind not in rem_ind]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing all our results into one table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column to add the matching names into a total table containing cd loan names\n",
    "#and corresopnding match names for each pre1790 loan times\n",
    "def mergeNames(colname, df):\n",
    "    loss_dict = dict(df.groupby('CD name')['Loan Office name'].apply(lambda x: list(x)))\n",
    "    cumulative_CD[colname + ' 1'] = cumulative_CD['full name 1'].apply(lambda name: loss_dict.get(name, np.nan))\n",
    "    cumulative_CD[colname + ' 2'] = cumulative_CD['full name 2'].apply(lambda name: loss_dict.get(name, np.nan))\n",
    "    cumulative_CD[colname + ' 3'] = cumulative_CD['full name 3'].apply(lambda name: loss_dict.get(name, np.nan))\n",
    "    cumulative_CD[colname] = pd.Series(cumulative_CD[[colname + ' 1', colname + ' 2', colname + ' 3']].values.tolist()).apply(lambda lst: [x for x in lst if type(x) != float])\n",
    "    cumulative_CD[colname] = cumulative_CD[colname].apply(lambda x: list(set(list(itertools.chain.from_iterable(x)))) if x != [] else np.nan)\n",
    "    cumulative_CD.drop([colname + ' 1', colname + ' 2', colname + ' 3'], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run process on all four pre1790 loan types\n",
    "mergeNames('Same State Loan Office', df_loanoffice_samestate)\n",
    "mergeNames('Different State Loan Office', df_loanoffice_difstate)\n",
    "mergeNames('Same State Liquidated Debt', df_samestateliquid)\n",
    "mergeNames('Different State Liquidated Debt', df_difstateliquid)\n",
    "mergeNames('Pierce Certificates', df_pierce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of loan types one person had\n",
    "cumulative_CD['tot_count'] = 5 - cumulative_CD[['Same State Loan Office',\n",
    "                                                'Different State Loan Office',\n",
    "                                                'Same State Liquidated Debt',\n",
    "                                                'Different State Liquidated Debt',\n",
    "                                                'Pierce Certificates']].isna().sum(axis = 1)\n",
    "cumulative_CD.drop(['3p_Cents','3p_Dollar', \n",
    "                    'full name 1','full name 2','full name 3'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some preprocesing - creating dictionaries to add back in data, after I turned the list of names into a string to remove duplicates\n",
    "#the original data we imported above was lost so we readd it by creating dictionaries\n",
    "cumulative_CD['str name'] = cumulative_CD['full name'].apply(lambda x: str(x)) + \"____\" + cumulative_CD['state']\n",
    "fullnamedict = dict(zip(cumulative_CD['str name'], cumulative_CD['full name']))\n",
    "statenamedict = dict(zip(cumulative_CD['str name'], cumulative_CD['state']))\n",
    "sslodict = dict(zip(cumulative_CD['str name'], cumulative_CD['Same State Loan Office']))\n",
    "dslodict = dict(zip(cumulative_CD['str name'], cumulative_CD['Different State Loan Office']))\n",
    "sslddict = dict(zip(cumulative_CD['str name'], cumulative_CD['Same State Liquidated Debt']))\n",
    "dslddict = dict(zip(cumulative_CD['str name'], cumulative_CD['Different State Liquidated Debt']))\n",
    "pcdict = dict(zip(cumulative_CD['str name'], cumulative_CD['Pierce Certificates']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use dictionaries to link data to matched values for each loan type\n",
    "cumulative_CD_assets = cumulative_CD.groupby('str name')['6p_Cents','6p_Dollar',\n",
    "                                                         '6p_def_Cents','6p_def_Dollar'].sum()\n",
    "cumulative_CD_assets.reset_index(inplace = True)\n",
    "cumulative_CD_assets['full name'] = cumulative_CD_assets['str name'].apply(lambda x: \n",
    "                                                                           fullnamedict[x])\n",
    "cumulative_CD_assets['state'] = cumulative_CD_assets['str name'].apply(lambda x: \n",
    "                                                                       statenamedict[x])\n",
    "cumulative_CD_assets['Same State Loan Office'] = cumulative_CD_assets['str name'].apply(lambda x: \n",
    "                                                                                        sslodict[x])\n",
    "cumulative_CD_assets['Different State Loan Office'] = cumulative_CD_assets['str name'].apply(lambda x: \n",
    "                                                                                             dslodict[x])\n",
    "cumulative_CD_assets['Same State Liquidated Debt'] = cumulative_CD_assets['str name'].apply(lambda x: \n",
    "                                                                                            sslddict[x])\n",
    "cumulative_CD_assets['Different State Liquidated Debt'] = cumulative_CD_assets['str name'].apply(lambda x: \n",
    "                                                                                                 dslddict[x])\n",
    "cumulative_CD_assets['Pierce Certificates'] = cumulative_CD_assets['str name'].apply(lambda x: \n",
    "                                                                                     pcdict[x])\n",
    "cumulative_CD_assets['Total'] = (cumulative_CD_assets['6p_def_Cents'] + \n",
    "                                 cumulative_CD_assets['6p_Cents'])/100 + (cumulative_CD_assets['6p_Dollar'] + \n",
    "                                                                          cumulative_CD_assets['6p_def_Dollar'])\n",
    "cumulative_CD_assets.drop(['6p_def_Cents', '6p_Cents', \n",
    "                           '6p_Dollar', '6p_def_Dollar', 'str name'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same State Loan Office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add asset counts for each individual on each row to the state loan office table for the same state merging method\n",
    "def ssloTotal(ind):\n",
    "    name_options = cumulative_CD_assets.loc[ind, 'Same State Loan Office']\n",
    "    state = cumulative_CD_assets.loc[ind, 'state']\n",
    "    state_office = loan_office[loan_office['State Name'] == state]\n",
    "    ind1 = state_office[state_office['Full Name 1'].apply(lambda x: x in name_options)].index.tolist()\n",
    "    ind2 = state_office[state_office['Full Name 2'].apply(lambda x: x in name_options)].index.tolist()\n",
    "    ind3 = state_office[state_office['Full Name 3'].apply(lambda x: x in name_options)].index.tolist()\n",
    "    ind1.extend(ind2)\n",
    "    ind1.extend(ind3)\n",
    "    total_val = state_office.loc[ind1]['Specie Value '].sum()\n",
    "    return total_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD_assets['SSLO Total'] = np.nan\n",
    "ssloIndex = cumulative_CD_assets[cumulative_CD_assets['Same State Loan Office'].apply(lambda x: type(x) == list)].index\n",
    "cumulative_CD_assets.loc[ssloIndex, 'SSLO Total'] = [ssloTotal(x) for x in ssloIndex]\n",
    "cumulative_CD_assets['SSLO Total'] = cumulative_CD_assets['SSLO Total'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different State Loan Office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add asset counts for each individual on each row to the state loan office table for the dif state merging method\n",
    "def dsloTotal(ind):\n",
    "    name_options = cumulative_CD_assets.loc[ind, 'Different State Loan Office']\n",
    "    state = cumulative_CD_assets.loc[ind, 'state']    \n",
    "    state_office = loan_office[loan_office['State Name'] != state]\n",
    "    ind1 = state_office[state_office['Full Name 1'].apply(lambda x: x in name_options)].index.tolist()\n",
    "    ind2 = state_office[state_office['Full Name 2'].apply(lambda x: x in name_options)].index.tolist()\n",
    "    ind3 = state_office[state_office['Full Name 3'].apply(lambda x: x in name_options)].index.tolist()\n",
    "    ind1.extend(ind2)\n",
    "    ind1.extend(ind3)\n",
    "    total_val = state_office.loc[ind1]['Specie Value '].sum()\n",
    "    return total_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD_assets['DSLO Total'] = np.nan\n",
    "dsloIndex = cumulative_CD_assets[cumulative_CD_assets['Different State Loan Office'].apply(lambda x: \n",
    "                                                                                           type(x) == list)].index\n",
    "cumulative_CD_assets.loc[dsloIndex, 'DSLO Total'] = [dsloTotal(x) for x in dsloIndex]\n",
    "cumulative_CD_assets['DSLO Total'] = cumulative_CD_assets['DSLO Total'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same State Liquidated Debt Certificates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add values of liquidated debt certificates\n",
    "def ssldTotal(ind):\n",
    "    name_options = cumulative_CD_assets.loc[ind, 'Same State Liquidated Debt']\n",
    "    state = cumulative_CD_assets.loc[ind, 'state']\n",
    "    \n",
    "    if state != 'PA' and state in state_name.keys():\n",
    "        num_names = state_name[state]\n",
    "        state_certs_file = 'Data/Pre1790/cleaned/liquidated_debt_certificates_'+state+'_cleaned.csv'\n",
    "        if exists(state_certs_file):\n",
    "            total_val = calculateTotalValue(state_certs_file, num_names, name_options)\n",
    "            return total_val\n",
    "    elif state == 'PA':\n",
    "        state_certs_file1 = 'Data/Pre1790/cleaned/liquidated_debt_certificates_PA_story_cleaned.csv'\n",
    "        total_val1 = calculateTotalValue(state_certs_file1, 1, name_options)\n",
    "        state_certs_file2 = 'Data/Pre1790/cleaned/liquidated_debt_certificates_PA_stelle_cleaned.csv'\n",
    "        total_val2 = calculateTotalValue(state_certs_file2, 2, name_options)\n",
    "        return total_val1 + total_val2\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate total value held by one person in debt certificates from a particular state\n",
    "def calculateTotalValue(file, num_names, name_options):\n",
    "    #this part is pretty similar to the merging part for liquidated debt certificates\n",
    "    state_cert = pd.read_csv(file, index_col = 0)\n",
    "    state_cert['Full Name'] = state_cert['First name'] + \" \" + state_cert['Last name'] \n",
    "    namelst = []\n",
    "    namelst.append('Full Name')\n",
    "    if num_names > 1:\n",
    "        for i in np.arange(2, num_names+1, 1):\n",
    "            fullname_str = 'Full Name ' + str(i)\n",
    "            state_cert[fullname_str] = state_cert['First name ' + str(i)] + \" \" + state_cert['Last name ' + str(i)] \n",
    "            namelst.append(fullname_str)\n",
    "    ind = []\n",
    "    for name in namelst:\n",
    "        ind.extend(state_cert[state_cert[name].apply(lambda x: x in name_options)].index.tolist())\n",
    "    #create subtable for the data we want, make it into a numeric value and sum it\n",
    "    subtbl = state_cert.loc[ind]\n",
    "    subtbl['Dollars'] = subtbl['Dollars'].apply(lambda x: float(x))\n",
    "    subtbl['90th'] = subtbl['90th'].apply(lambda x: float(x) if x != '22/8' else 22/8)\n",
    "    total_val = subtbl['Dollars'].sum() + subtbl['90th'].sum()/90\n",
    "    return total_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD_assets['SSLD Total'] = np.nan\n",
    "ssldIndex = cumulative_CD_assets[cumulative_CD_assets['Same State Liquidated Debt'].apply(lambda x: \n",
    "                                                                                          type(x) == list)].index\n",
    "cumulative_CD_assets.loc[ssldIndex, 'SSLD Total'] = [ssldTotal(x) for x in ssldIndex]\n",
    "cumulative_CD_assets['SSLD Total'] = cumulative_CD_assets['SSLD Total'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different State Liquidated Debt Certificates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add values of liquidated debt certificates\n",
    "def dsldTotal(ind):\n",
    "    name_options = cumulative_CD_assets.loc[ind, 'Different State Liquidated Debt']\n",
    "    state = cumulative_CD_assets.loc[ind, 'state']\n",
    "    sumval = 0\n",
    "    for statename in states:\n",
    "        if statename != 'PA' and not pd.isnull(state_name[statename]):\n",
    "            num_names = state_name[statename]\n",
    "            state_certs_file = 'Data/Pre1790/cleaned/liquidated_debt_certificates_'+statename+'_cleaned.csv'\n",
    "            if exists(state_certs_file):\n",
    "                total_val = calculateTotalValue(state_certs_file, \n",
    "                                                num_names, name_options)\n",
    "            sumval = sumval + total_val\n",
    "        elif statename == 'PA':\n",
    "            state_certs_file1 = 'Data/Pre1790/cleaned/liquidated_debt_certificates_PA_story_cleaned.csv'\n",
    "            total_val1 = calculateTotalValue(state_certs_file1, \n",
    "                                             1, name_options)\n",
    "            state_certs_file2 = 'Data/Pre1790/cleaned/liquidated_debt_certificates_PA_stelle_cleaned.csv'\n",
    "            total_val2 = calculateTotalValue(state_certs_file2, \n",
    "                                             2, name_options)\n",
    "            sumval = sumval + total_val1 + total_val2\n",
    "    return sumval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate total value held by one person in debt certificates from a particular state\n",
    "def calculateTotalValue(file, num_names, name_options):\n",
    "    #this part is pretty similar to the merging part for liquidated debt certificates\n",
    "    state_cert = pd.read_csv(file, index_col = 0)\n",
    "    state_cert['Full Name'] = state_cert['First name'] + \" \" + state_cert['Last name'] \n",
    "    namelst = []\n",
    "    namelst.append('Full Name')\n",
    "    if num_names > 1:\n",
    "        for i in np.arange(2, num_names+1, 1):\n",
    "            fullname_str = 'Full Name ' + str(i)\n",
    "            state_cert[fullname_str] = state_cert['First name ' + str(i)] + \" \" + state_cert['Last name ' + str(i)] \n",
    "            namelst.append(fullname_str)\n",
    "    ind = []\n",
    "    for name in namelst:\n",
    "        ind.extend(state_cert[state_cert[name].apply(lambda x: x in name_options)].index.tolist())\n",
    "    #create subtable for the data we want, make it into a numeric value and sum it\n",
    "    subtbl = state_cert.loc[ind]\n",
    "    subtbl['Dollars'] = subtbl['Dollars'].apply(lambda x: float(x))\n",
    "    subtbl['90th'] = subtbl['90th'].apply(lambda x: float(x) if x != '22/8' else 22/8)\n",
    "    total_val = subtbl['Dollars'].sum() + subtbl['90th'].sum()/90\n",
    "    return total_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD_assets['DSLD Total'] = np.nan\n",
    "dsldIndex = cumulative_CD_assets[cumulative_CD_assets['Different State Liquidated Debt'].apply(lambda x:\n",
    "                                                                                               type(x) == list)].index\n",
    "cumulative_CD_assets.loc[dsldIndex, 'DSLD Total'] = [dsldTotal(x) for x in dsldIndex]\n",
    "cumulative_CD_assets['DSLD Total'] = cumulative_CD_assets['DSLD Total'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same State Pierce Certificates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate sum for pierce certificates\n",
    "def pcTotal(ind):\n",
    "    name_options = cumulative_CD_assets.loc[ind, 'Pierce Certificates']\n",
    "    state = cumulative_CD_assets.loc[ind, 'state']\n",
    "    \n",
    "    pierce_state = pierce[pierce['State'].apply(lambda x: \n",
    "                                                pd.isnull(x) or x == state)]\n",
    "    ind1 = pierce_state[pierce_state['Full Name'].apply(lambda x: \n",
    "                                                        x in name_options)].index.tolist()\n",
    "    ind2 = pierce_state[pierce_state['Full Name 2'].apply(lambda x: \n",
    "                                                          x in name_options)].index.tolist()\n",
    "    ind1.extend(ind2)\n",
    "    total_val = pierce_state.loc[ind1]['Value'].sum()\n",
    "    return total_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD_assets['PC Total'] = np.nan\n",
    "pcIndex = cumulative_CD_assets[cumulative_CD_assets['Pierce Certificates'].apply(lambda x: \n",
    "                                                                                 type(x) == list)].index\n",
    "cumulative_CD_assets.loc[pcIndex, 'PC Total'] = [pcTotal(x) for x in pcIndex]\n",
    "cumulative_CD_assets['PC Total'] = cumulative_CD_assets['PC Total'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD_assets['Debt Total'] = cumulative_CD_assets[['SSLO Total','DSLO Total',\n",
    "                                                           'SSLD Total', 'DSLD Total',\n",
    "                                                           'PC Total']].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre1790_certs = ['Same State Loan Office','Different State Loan Office', \n",
    "                 'Same State Liquidated Debt','Different State Liquidated Debt','Pierce Certificates']\n",
    "cumulative_CD_assets['tot_pre1790_certs'] = 5 - cumulative_CD_assets[pre1790_certs].isna().sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD_assets.to_csv(\"prepost_matched_debt_files.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
