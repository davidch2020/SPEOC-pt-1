{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data Cleaning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rapidfuzz import process\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that makes dictionary that combines 3 full name columns into 1\n",
    "def genFullNameDict(df):    \n",
    "    namemerge_pre = dict()\n",
    "    for x in df.index:\n",
    "        marker = False\n",
    "        row = df.loc[x][['full name 1', 'full name 2', 'full name 3']]\n",
    "        #identify number of unique names\n",
    "        if len(row[row.notna()].unique()) > 1:\n",
    "            row_names = row[row.notna()].unique()\n",
    "            for name in row_names:\n",
    "                if name in namemerge_pre.keys() and not marker:\n",
    "                    #add to dictionary if entry already exists\n",
    "                    namemerge_pre[name].extend([n for n in row_names if n != name and n not in namemerge_pre[name]])\n",
    "                    marker = True\n",
    "            #add to dictionary using shortest name as key\n",
    "            if not marker:\n",
    "                minname = row_names[0]\n",
    "                for name in row_names:\n",
    "                    if len(name) < len(minname):\n",
    "                        minname = name\n",
    "                namemerge_pre[minname] = [n for n in row_names if n != minname]\n",
    "    \n",
    "    #invert the dictionary - swap keys and values\n",
    "    namemerge = dict()\n",
    "    for key in namemerge_pre.keys():\n",
    "        vals = namemerge_pre[key]\n",
    "        #iterate through list of values\n",
    "        for val in vals:\n",
    "            namemerge[val] = key\n",
    "    \n",
    "    return namemerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the fullname 1/2/3 columns to generate full name column\n",
    "def setFullName(df):\n",
    "    for x in df.index:\n",
    "        marker = False\n",
    "        row = df.loc[x][['full name 1', 'full name 2', 'full name 3']]\n",
    "        if len(row[row.notna()].unique()) == 0:\n",
    "            df.loc[x, 'full name'] = np.nan\n",
    "        else:\n",
    "            df.loc[x, 'full name'] = row[row.notna()].unique()[0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genFuzzyDict(df):\n",
    "    namelst = df['full name'].drop_duplicates()\n",
    "    fn_fuzzy_pre = dict()\n",
    "    for name in namelst:\n",
    "        marker = False\n",
    "        if not pd.isnull(name):\n",
    "            #find matches for name\n",
    "            match = process.extract(name, [x for x in namelst if x != name and not pd.isnull(x)], limit = 1)[0]\n",
    "            if match[1]>90:\n",
    "                #add suitable matches to dictionary\n",
    "                for nm in [match[0], name]:\n",
    "                    if nm in fn_fuzzy_pre.keys() and not marker:\n",
    "                        fn_fuzzy_pre[nm].extend([n for n in [match[0], name] if n != nm and n not in fn_fuzzy_pre[nm]])\n",
    "                        marker = True\n",
    "                if not marker:\n",
    "                    if len(name) < len(match[0]):\n",
    "                        fn_fuzzy_pre[name] = [match[0]]\n",
    "                    else:\n",
    "                        fn_fuzzy_pre[match[0]] = [name]\n",
    "    #invert dictionary\n",
    "    fn_fuzzy = dict()\n",
    "    for key in fn_fuzzy_pre.keys():\n",
    "        vals = fn_fuzzy_pre[key]\n",
    "        for val in vals:\n",
    "            fn_fuzzy[val] = key\n",
    "    \n",
    "    return fn_fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformdf(df, state):\n",
    "    df['full name 1'] = df['First Name'] + \" \" + df['Last Name']\n",
    "    df['full name 2'] = df['First Name.1'] + \" \" + df['Last Name.1']\n",
    "    df['full name 3'] = df['First Name.2'].apply(lambda x: x if type(x) == str else \"\") + \" \" + df['Last Name.2']\n",
    "    df['state'] = state\n",
    "    \n",
    "    namemerge = genFullNameDict(df)\n",
    "    for col in ['full name 1', 'full name 2', 'full name 3']:    \n",
    "        df[col] = df[col].apply(lambda x: namemerge[x] if x in namemerge.keys() else x)\n",
    "\n",
    "    df['full name'] = np.nan\n",
    "    df = setFullName(df)\n",
    "\n",
    "    fn_fuzzy = genFuzzyDict(df)\n",
    "    df['full name'] = df['full name'].apply(lambda x: x if x not in fn_fuzzy.keys() else fn_fuzzy[x])\n",
    "    \n",
    "    df_agg = df[['full name', 'state', '6p_Dollar', '6p_Cents', '6p_def_Dollar', '6p_def_Cents', '3p_Dollar', '3p_Cents']]\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformonecoldf(df, state):\n",
    "    df['full name'] =  df['First Name'] + \" \" + df['Last Name']\n",
    "    df['state'] = state\n",
    "    fn_fuzzy = genFuzzyDict(df)\n",
    "    df['full name'] = df['full name'].apply(lambda x: x if x not in fn_fuzzy.keys() else fn_fuzzy[x])    \n",
    "    df_agg = df[['full name', 'state', '6p_Dollar', '6p_Cents', '6p_def_Dollar', '6p_def_Cents', '3p_Dollar', '3p_Cents']]\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecticut Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "CT_CD = pd.read_excel(\"Data/Post1790/CT/CT_post1790_CD_ledger.xlsx\", header = 13, usecols = 'H, I, N, O, X, Y, AD, AE, AN, AO, AT, AU')\n",
    "CT_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents', 'First Name.1', 'Last Name.1', '6p_def_Dollar', '6p_def_Cents',\n",
    "                  'First Name.2', 'Last Name.2', '3p_Dollar', '3p_Cents']\n",
    "CT_CD_agg = transformdf(CT_CD, 'CT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maryland Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "MD_CD = pd.read_excel(\"Data/Post1790/MD/MD_post1790_CD.xlsx\", header = 11, usecols = 'G, H, L, M, U, V, Z, AA, AI, AJ, AN, AO')\n",
    "MD_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents', 'First Name.1', 'Last Name.1', '6p_def_Dollar', '6p_def_Cents',\n",
    "                  'First Name.2', 'Last Name.2', '3p_Dollar', '3p_Cents']\n",
    "MD_CD_agg = transformdf(MD_CD, 'MD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([CT_CD_agg, MD_CD_agg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# North Carolina Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "NC_CD = pd.read_excel(\"Data/Post1790/NC/T695_R4_NC_CD.xlsx\", header = 11, usecols = 'J, K, W, X, Z, AA, AC, AD ')\n",
    "NC_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents', '6p_def_Dollar', '6p_def_Cents', '3p_Dollar', '3p_Cents']\n",
    "NC_CD_agg = transformonecoldf(NC_CD, 'NC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([NC_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Hampshire Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "NH_CD = pd.read_excel(\"Data/Post1790/NH/T652_R6_New_Hampshire_CD.xlsx\", header = 10, usecols = 'I, J, N, O, P, Q, R, S')\n",
    "NH_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents',  '6p_def_Dollar', '6p_def_Cents', '3p_Dollar', '3p_Cents']\n",
    "NH_CD_agg = transformonecoldf(NH_CD, 'NH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([NH_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "NY_CD = pd.read_excel(\"Data/Post1790/NY/NY_1790_CD.xlsx\", header = 11, usecols = 'H, I, M, N, X, Y, AC, AD, AM, AN, AR, AS')\n",
    "NY_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents', 'First Name.1', 'Last Name.1', '6p_def_Dollar', '6p_def_Cents',\n",
    "                  'First Name.2', 'Last Name.2', '3p_Dollar', '3p_Cents']\n",
    "NY_CD_agg = transformdf(NY_CD, 'NY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([NY_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# South Carolina Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "SC_CD = pd.read_excel(\"Data/Post1790/SC/Post_1790_South_Carolina_CD.xlsx\", header = 11, usecols = 'D, E, M, N, S, T, AB, AC, AH, AI, AQ, AR')\n",
    "SC_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents', 'First Name.1', 'Last Name.1', '6p_def_Dollar', '6p_def_Cents',\n",
    "                  'First Name.2', 'Last Name.2', '3p_Dollar', '3p_Cents']\n",
    "SC_CD_agg = transformdf(SC_CD, 'SC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([SC_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pennsylvania Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "PA_CD = pd.read_excel(\"Data/Post1790/PA/PA_post1790_CD.xlsx\", header = 11, usecols = 'G, H, L, M, U, V, Z, AA, AI, AJ, AO, AP')\n",
    "PA_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents', 'First Name.1', 'Last Name.1', '6p_def_Dollar', '6p_def_Cents',\n",
    "                  'First Name.2', 'Last Name.2', '3p_Dollar', '3p_Cents']\n",
    "PA_CD_agg = transformdf(PA_CD, 'PA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([PA_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rhode Island Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "RI_CD = pd.read_excel(\"Data/Post1790/RI/T653_Rhode_Island_CD.xlsx\", header = 11, usecols = 'G, H, L, M, U, V, Z, AA, AI, AJ, AN, AO')\n",
    "RI_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents', 'First Name.1', 'Last Name.1', '6p_def_Dollar', '6p_def_Cents',\n",
    "                  'First Name.2', 'Last Name.2', '3p_Dollar', '3p_Cents']\n",
    "RI_CD_agg = transformdf(RI_CD, 'RI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([RI_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virginia Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "VA_CD = pd.read_excel(\"Data/Post1790/VA/VA_CD.xlsx\", header = 11, usecols = 'H, I, K, L, U, V, X, Y, AH, AI, AK, AL')\n",
    "VA_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents', 'First Name.1', 'Last Name.1', '6p_def_Dollar', '6p_def_Cents',\n",
    "                  'First Name.2', 'Last Name.2', '3p_Dollar', '3p_Cents']\n",
    "VA_CD_agg = transformdf(VA_CD, 'VA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([VA_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Georgia Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "GA_CD = pd.read_excel(\"Data/Post1790/GA/T694_GA_Loan_Office_CD.xlsx\", header = 10, usecols = 'Q, R, Z, AA, AB, AC, AD, AE')\n",
    "GA_CD.columns = ['First Name', 'Last Name', '6p_Dollar', '6p_Cents',  '6p_def_Dollar', '6p_def_Cents', '3p_Dollar', '3p_Cents']\n",
    "GA_CD_agg = transformonecoldf(GA_CD, 'GA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([GA_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Jersey Continental Debt Dataset Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare loan dataset\n",
    "NJ_CD = pd.read_excel(\"Data/Post1790/NJ/NJ_3_percent_stock_T698_R1_R2.xlsx\", header = 11, usecols = 'D, E, L, M')\n",
    "NJ_CD.columns = ['First Name', 'Last Name', '3p_Dollar', '3p_Cents']\n",
    "NJ_CD['full name'] =  NJ_CD['First Name'] + \" \" + NJ_CD['Last Name']\n",
    "NJ_CD['state'] = 'NJ'\n",
    "fn_fuzzy = genFuzzyDict(NJ_CD)\n",
    "NJ_CD['full name'] = NJ_CD['full name'].apply(lambda x: x if x not in fn_fuzzy.keys() else fn_fuzzy[x])    \n",
    "NJ_CD_agg = NJ_CD[['full name', 'state', '3p_Dollar', '3p_Cents']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD = pd.concat([NJ_CD_agg, cumulative_CD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_CD.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many unique individuals were issued 6 percent stocks or deferred 6 percent stocks in 1790 and after?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table of number of unique individuals issued 6% stocks (normal or deferred) by state\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "state\n",
       "CT    705\n",
       "GA     54\n",
       "MD    333\n",
       "NC     53\n",
       "NH    169\n",
       "NY    158\n",
       "PA    285\n",
       "RI    409\n",
       "SC    256\n",
       "VA    421\n",
       "Name: full name, dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_6 = cumulative_CD[['6p_Dollar', '6p_Cents', '6p_def_Dollar', '6p_def_Cents']].dropna(thresh = 1).index\n",
    "print('table of number of unique individuals issued 6% stocks (normal or deferred) by state')\n",
    "cumulative_CD[cumulative_CD['full name'].apply(lambda x: not pd.isnull(x))].loc[stocks_6].groupby('state')['full name'].apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many of these individuals\n",
    "- were original purchasers of loan office certicates of the same state as the 6 percent stock?\n",
    "- were original purchasers of loan office certicates issued from another state?\n",
    "- were original recipients of liquidated debtcertiâ€‚cates issued by the same-state loan office? other state loan offices?\n",
    "- were original recipients of the Pierce Certicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_merge(lst1, lst2, threshold=85, limit = 100):\n",
    "    \"\"\"\n",
    "    :param df_1: the left list to join\n",
    "    :param df_2: the right list to join\n",
    "    :param threshold: how close the matches should be to return a match, based on Levenshtein distance\n",
    "    :param limit: the amount of matches that will get returned, these are sorted high to low\n",
    "    :return: dataframe with boths keys and matches\n",
    "    \"\"\"\n",
    "    \n",
    "    delegates = pd.Series([x for x in lst1.unique() if not pd.isnull(x)])\n",
    "    possible =  [x for x in lst2.unique().tolist() if type(x) == str]\n",
    "\n",
    "    \n",
    "    #get matches\n",
    "    #process.extract uses a combination of all four fuzzywuzzy scores\n",
    "    matches = delegates.apply(lambda x: process.extract(x, possible, limit=limit, score_cutoff = threshold))\n",
    "    \n",
    "    match_df = pd.DataFrame(columns = ['Delegates', 'Loan Matches'])\n",
    "    \n",
    "    for delegate, matchset in zip(delegates, matches):\n",
    "        matchset_thres = [name for name in matchset if name[1] >= threshold]\n",
    "        if len(matchset_thres) == 0:\n",
    "            add_df = pd.DataFrame(data = {'Delegates': [delegate], 'Loan Matches': [\"\"], 'Scores': [0]})\n",
    "            match_df = pd.concat([match_df, add_df])\n",
    "        else:\n",
    "            delegate_lst = [delegate] * len(matchset_thres)\n",
    "            add_df = pd.DataFrame(data = {'Delegates': delegate_lst, \n",
    "                                          'Loan Matches': [x[0] for x in matchset_thres],\n",
    "                                          'Scores': [x[1] for x in matchset_thres]})\n",
    "            match_df = pd.concat([match_df, add_df])\n",
    "\n",
    "    return match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for performing the second step of the match\n",
    "def matchFunction(lst1, lst2, score = 90):\n",
    "    lst1 = list(set(lst1))\n",
    "    lst2 = list(set(lst2))\n",
    "    threshold = min(len(lst1), len(lst2))\n",
    "    matches = 0\n",
    "    for wd1 in lst1:\n",
    "        for wd2 in lst2:\n",
    "            if process.extract(wd1, [wd2])[0][1] > score:\n",
    "                matches+=1\n",
    "    return matches >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produceMatches(delegates, debt, delegate_names, debt_names):\n",
    "    initial = True\n",
    "    join_df = pd.DataFrame()\n",
    "    for del_name in delegate_names:\n",
    "        for debt_name in debt_names:\n",
    "            if initial:\n",
    "                join_df = fuzzy_merge(delegates[del_name], debt[debt_name])\n",
    "                initial = False\n",
    "            else:\n",
    "                add_df = fuzzy_merge(delegates[del_name], debt[debt_name])\n",
    "                join_df = pd.concat([join_df, add_df])\n",
    "    join_df = join_df.drop_duplicates().reset_index(drop = True)\n",
    "    join_df = join_df[join_df['Scores'].apply(lambda x: x != 0)]\n",
    "    join_df = join_df[join_df['Loan Matches'].apply(lambda x: not pd.isnull(x))]    \n",
    "    \n",
    "    join_df_p2 = join_df[join_df['Loan Matches'].apply(lambda x: len(list(set(x.replace(\"??\", \"\").strip().split(\" \"))))>=2)]\n",
    "    join_df_p2_final = join_df_p2[[matchFunction(x.split(\" \"), y.split(\" \")) for x, y in zip(join_df_p2['Delegates'], join_df_p2['Loan Matches'])]]\n",
    "    return join_df_p2_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringConvert(x):\n",
    "    return x if type(x) == str else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_office = pd.read_csv('Data/Pre1790/cleaned/loan_office_certificates_9_states_cleaned.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['NH', 'MA', 'CT', 'NY', 'NJ', 'PA', 'DE', 'MD', 'VA']\n",
    "num_names = [1, 2, 2, 3, 2, None, 2, None, None]\n",
    "state_names = dict(zip(np.arange(1, 10, 1), states))\n",
    "loan_office['State Name'] = loan_office['State'].apply(lambda x: state_names[x])\n",
    "loan_office['Full Name 1'] = (loan_office['First Name 1 '].apply(lambda x: stringConvert(x)) + \" \" + loan_office['Last Name 1 '].apply(lambda x: stringConvert(x))).apply(lambda x: np.nan if x.strip() == \"\" else x)\n",
    "loan_office['Full Name 2'] = (loan_office['First Name 2'].apply(lambda x: stringConvert(x)) + \" \" + loan_office['Last Name 2'].apply(lambda x: stringConvert(x))).apply(lambda x: np.nan if x.strip() == \"\" else x)\n",
    "loan_office['Full Name 3'] = (loan_office['First Name 3'].apply(lambda x: stringConvert(x)) + \" \" + loan_office['Last Name 3'].apply(lambda x: stringConvert(x))).apply(lambda x: np.nan if x.strip() == \"\" else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many individuals were original purchasers of loan office certicates of the same state as the 6 percent stock?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loanOfficeSameState(state):\n",
    "    state_ind = cumulative_CD[cumulative_CD['state'] == state][['6p_Dollar', '6p_Cents', '6p_def_Dollar', '6p_def_Cents']].dropna(thresh = 1).index\n",
    "    if len(state_ind) != 0:\n",
    "        state_cd = cumulative_CD.loc[state_ind][['full name', 'state']].drop_duplicates()\n",
    "        state_cd[state_cd['full name'].apply(lambda x: len(x.strip().split(\" \"))>1 if not pd.isnull(x) else False)]\n",
    "        state_cd.columns = ['cd name 1', 'cd state']\n",
    "        loan_office_state = loan_office[loan_office['State Name'] == state][['Full Name 1', 'Full Name 2', 'Full Name 3', 'State Name']].drop_duplicates()\n",
    "        loan_office_state.columns = ['loan office name 1', 'loan office name 2', 'loan office name 3', 'loan office state']\n",
    "        matches = produceMatches(state_cd, loan_office_state, delegate_names = ['cd name 1'], debt_names = ['loan office name 1', 'loan office name 2', 'loan office name 3'])\n",
    "        return len(set(matches['Delegates']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeStateResult(result, dictionary, key):\n",
    "    dictionary[key] = result\n",
    "    print(key, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many individuals were original purchasers of loan office certicates issued from another state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loanOfficeDifState(state):\n",
    "    state_ind = cumulative_CD[cumulative_CD['state'] == state][['6p_Dollar', '6p_Cents', '6p_def_Dollar', '6p_def_Cents']].dropna(thresh = 1).index\n",
    "    if len(state_ind) != 0:\n",
    "        state_cd = cumulative_CD.loc[state_ind][['full name', 'state']].drop_duplicates()\n",
    "        state_cd[state_cd['full name'].apply(lambda x: len(x.strip().split(\" \"))>1 if not pd.isnull(x) else False)]\n",
    "        state_cd.columns = ['cd name 1', 'cd state']\n",
    "        loan_office_nostate = loan_office[loan_office['State Name'] != state][['Full Name 1', 'Full Name 2', 'Full Name 3', 'State Name']].drop_duplicates()\n",
    "        loan_office_nostate.columns = ['loan office name 1', 'loan office name 2', 'loan office name 3', 'loan office state']\n",
    "        matches = produceMatches(state_cd, loan_office_nostate, delegate_names = ['cd name 1'], debt_names = ['loan office name 1', 'loan office name 2', 'loan office name 3'])\n",
    "        return len(set(matches['Delegates']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NH 44\n",
      "MA None\n",
      "CT 109\n",
      "NY 27\n",
      "NJ None\n",
      "PA 26\n",
      "DE None\n",
      "MD 63\n",
      "VA 90\n"
     ]
    }
   ],
   "source": [
    "for state in states:\n",
    "    print(state, loanOfficeDifState(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liquidatedStateDebt(state, file, num_names, df_ind = False):\n",
    "    state_ind = cumulative_CD[cumulative_CD['state'] == state][['6p_Dollar', '6p_Cents']].dropna(thresh = 1).index\n",
    "    if len(state_ind) != 0:\n",
    "        state_cd = cumulative_CD.loc[state_ind][['full name', 'state']].drop_duplicates()\n",
    "        state_cd[state_cd['full name'].apply(lambda x: len(x.strip().split(\" \"))>1 if not pd.isnull(x) else False)]\n",
    "        state_cd.columns = ['cd name 1', 'cd state']\n",
    "        datafile = 'Data/Pre1790/cleaned/'+file\n",
    "        if exists(datafile):\n",
    "            state_cert = pd.read_csv(datafile, index_col = 0)\n",
    "            namelst = []\n",
    "            state_cert['Full Name'] = state_cert['First name'] + \" \" + state_cert['Last name'] \n",
    "            namelst.append('Full Name')\n",
    "            if num_names > 1:\n",
    "                for i in np.arange(2, num_names+1, 1):\n",
    "                    fullname_str = 'Full Name ' + str(i)\n",
    "                    state_cert[fullname_str] = state_cert['First name ' + str(i)] + \" \" + state_cert['Last name ' + str(i)] \n",
    "                    namelst.append(fullname_str)\n",
    "            state_cert_names = state_cert[namelst].drop_duplicates()\n",
    "            matches = produceMatches(state_cd, state_cert_names, delegate_names = ['cd name 1'], debt_names = namelst)\n",
    "            if df_ind:\n",
    "                return matches\n",
    "            return len(set(matches['Delegates']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_liquid_results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Full Name'] NH\n",
      "['Full Name', 'Full Name 2'] CT\n",
      "['Full Name', 'Full Name 2', 'Full Name 3'] NY\n",
      "['Full Name'] PA\n",
      "['Full Name', 'Full Name 2'] PA\n"
     ]
    }
   ],
   "source": [
    "for state, num_name in zip(states, num_names):\n",
    "    if state != \"PA\":\n",
    "        file = 'liquidated_debt_certificates_'+state+'_cleaned.csv'\n",
    "        result = liquidatedStateDebt(state, file, num_name)\n",
    "    else:\n",
    "        file1 = 'liquidated_debt_certificates_PA_story_cleaned.csv'\n",
    "        df1 = liquidatedStateDebt('PA', file1, 1, df_ind = True)\n",
    "        file2 = 'liquidated_debt_certificates_PA_stelle_cleaned.csv'\n",
    "        df2 = liquidatedStateDebt('PA', file2, 2, df_ind = True)\n",
    "        df = pd.concat([df1, df2])\n",
    "        result = len(set(df['Delegates']))\n",
    "        state_liquid_results[state] = result\n",
    "    state_liquid_results[state] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NH': 28,\n",
       " 'MA': None,\n",
       " 'CT': 100,\n",
       " 'NY': 38,\n",
       " 'NJ': None,\n",
       " 'PA': 23,\n",
       " 'DE': None,\n",
       " 'MD': None,\n",
       " 'VA': None}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_liquid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
