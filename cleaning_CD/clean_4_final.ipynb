{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# define dictionary to convert between different naming schematics\n",
    "statedict = {'PA': 'Pennsylvania', 'CT': 'Connecticut', 'MA': 'Massachusetts', 'NH': 'New Hampshire', 'DE': 'Delaware',\n",
    "             'NC': 'North Carolina', 'GA': 'Georgia', 'NY': 'New York', 'NJ': 'New Jersey', 'RI': 'Rhode Island',\n",
    "             'VA': 'Virginia', 'MD': 'Maryland', 'SC': 'South Carolina', 'VT': 'Vermont'}\n",
    "statedict_rev = dict(zip(statedict.values(), statedict.keys()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def parseLocationString(location, nametype):\n",
    "    \"\"\"\n",
    "    function to parse location string that I can use when cleaning data\n",
    "\n",
    "    :param location: string contaiing information about town county state and name_type separated by |\n",
    "    :param nametype: type of location\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if nametype == 'town':\n",
    "        return location.split(\" | \")[0], location.split(\" | \")[1], location.split(\" | \")[2], location.split(\" | \")[3]\n",
    "    elif nametype == 'county':\n",
    "        return \"\", location.split(\" | \")[0], location.split(\" | \")[1], location.split(\" | \")[2]\n",
    "    else:  #nametype == 'state'\n",
    "        return \"\", \"\", location.split(\" | \")[0], location.split(\" | \")[1]\n",
    "\n",
    "\n",
    "def tNameList(lst):\n",
    "    \"\"\"\n",
    "    takes a list of names and returns a string of names separated by \" | \", sorted and with duplicates removed, and with \"\" removed\n",
    "    :param lst: input lst\n",
    "    :return: string with names joined\n",
    "    \"\"\"\n",
    "    return \" | \".join(sorted(list(set([ele for ele in lst if ele != \"\"]))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# aggregated debt data\n",
    "CD_clean = pd.read_csv('../data_clean/aggregated_CD_post1790.csv', index_col=0).fillna(\"\").drop_duplicates()\n",
    "# all names that were scraped by the scraper\n",
    "scraped_names = pd.read_csv('scrape_tools/name_list_scraped.csv', index_col=0).fillna(\"\").drop_duplicates()\n",
    "# results of scraping\n",
    "match_df = pd.read_csv('scrape_tools/scrape_results.csv', index_col=0).fillna(\"\").drop_duplicates()\n",
    "# table that organizes results of scraping based off our list of names\n",
    "name_df = pd.read_csv('clean_tools/name_list.csv', index_col=0).fillna(\"\").drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# sometimes, an occupation is listed in the name but is not in the occupation column. we want to correct for this by adding the occupaiton title into the occupation column\n",
    "# do this for treasurers\n",
    "CD_clean.loc[CD_clean[CD_clean['Name'].apply(lambda x: 'treasurer' in x.lower())].index, 'occupation'] = [\n",
    "    ele if ele != '' else 'Treasurer' for ele in\n",
    "    CD_clean.loc[CD_clean[CD_clean['Name'].apply(lambda x: 'treasurer' in x.lower())].index, 'occupation']]\n",
    "# do this for administrators\n",
    "CD_clean.loc[\n",
    "    CD_clean[CD_clean['Name'].apply(lambda x: ' adm' in x.lower() or 'adm ' in x.lower())].index, 'occupation'] = [\n",
    "    ele if ele != '' else 'Administrator' for ele in CD_clean.loc[\n",
    "        CD_clean[CD_clean['Name'].apply(lambda x: ' adm' in x.lower() or 'adm ' in x.lower())].index, 'occupation']]\n",
    "# do this for trustees\n",
    "CD_clean.loc[CD_clean[CD_clean['Name'].apply(lambda x: ' trust ' in x.lower())].index, 'occupation'] = [\n",
    "    ele if ele != '' else 'Administrator' for ele in\n",
    "    CD_clean.loc[CD_clean[CD_clean['Name'].apply(lambda x: ' trust ' in x.lower())].index, 'occupation']]\n",
    "# do this for guardians\n",
    "CD_clean.loc[CD_clean[CD_clean['Name'].apply(lambda x: 'guard' in x.lower())].index, 'occupation'] = [\n",
    "    ele if ele != 'Yeoman' else 'Guardian | Yeoman' for ele in\n",
    "    CD_clean.loc[CD_clean[CD_clean['Name'].apply(lambda x: 'guard' in x.lower())].index, 'occupation']]\n",
    "# do this for school committee members\n",
    "CD_clean.loc[CD_clean[CD_clean['Name'].apply(lambda x: 'school' in x.lower())].index, 'occupation'] = [\n",
    "    ele if ele != '' else 'School Committee' for ele in\n",
    "    CD_clean.loc[CD_clean[CD_clean['Name'].apply(lambda x: 'school' in x.lower())].index, 'occupation']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# add in both names in cases where one row is identified with multiple name\n",
    "CD_merged = pd.merge(CD_clean, name_df,\n",
    "                     how='left',\n",
    "                     on=['Name', 'new_town', 'county', 'new_state', 'country', 'name_type', ])\n",
    "# add in match index and status\n",
    "CD_merged_mind = pd.merge(CD_merged,\n",
    "                          scraped_names[\n",
    "                              ['Fn_Fix', 'Ln_Fix', 'new_town', 'county', 'new_state', 'country', 'name_type', 'url',\n",
    "                               'Match Index', 'Match Status']],\n",
    "                          how='left',\n",
    "                          on=['Fn_Fix', 'Ln_Fix', 'new_town', 'county', 'new_state', 'country', 'name_type'])\n",
    "# combine names\n",
    "CD_merged_mind['Full Search Name'] = CD_merged_mind['Fn_Fix'] + ' ' + CD_merged_mind['Ln_Fix']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5s/dvrxt95949x1pm_sjxm85lj00000gn/T/ipykernel_51442/2871475831.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group_name_df['Rep Name'] = group_name_df['Full Search Name'].apply(lambda x: max(x, key=len))\n"
     ]
    }
   ],
   "source": [
    "# find names that are associated with multiple search names because they're actually the same name, by grouping based on match index (have same match index)\n",
    "# these names are associated with multiple names because of spelling variation\n",
    "# remove names with \"\" Match Index - too many\n",
    "grouped_names = CD_merged_mind[CD_merged_mind['Match Index'] != \"\"].groupby('Match Index').agg(\n",
    "    {'Full Search Name': lambda x: list(set(x))}).reset_index()\n",
    "group_name_df = grouped_names[grouped_names['Full Search Name'].apply(lambda x: len(x) > 1)]\n",
    "# denote the name we use moving forward as the name with the longest length (most information)\n",
    "group_name_df['Rep Name'] = group_name_df['Full Search Name'].apply(lambda x: max(x, key=len))\n",
    "group_name_df = group_name_df.explode('Full Search Name').reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# some manual corrections of the actual name we want to use for grouping\n",
    "group_name_df.loc[group_name_df[group_name_df['Full Search Name'].apply(\n",
    "    lambda x: 'Israel Joseph' in x)].index, 'Rep Name'] = 'Israel Joseph'\n",
    "group_name_df.loc[group_name_df[group_name_df['Full Search Name'].apply(\n",
    "    lambda x: 'William Larned' in x or 'William Learned' in x)].index, 'Rep Name'] = 'William Larned'\n",
    "group_name_df.loc[group_name_df[group_name_df['Full Search Name'].apply(\n",
    "    lambda x: 'Mathew Watson' in x or 'Matthew Watson' in x)].index, 'Rep Name'] = 'Mathew Watson'\n",
    "group_name_df.drop(group_name_df[group_name_df['Rep Name'] + group_name_df[\n",
    "    'Full Search Name'] == 'Samuel Vernon 2NdSamuel Vernon Ii'].index, inplace=True)\n",
    "group_name_df.drop(group_name_df[group_name_df['Rep Name'] + group_name_df[\n",
    "    'Full Search Name'] == 'Thomas Cloyd HalseyThomas Lloyd Halsey'].index, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# we want to group all the mispelled names together under the same name, and set their information to be the same as the name that is representing them - the rep name\n",
    "# first we find all names that are being grouped under a name that is not theirs\n",
    "group_name_df = group_name_df[group_name_df['Full Search Name'] != group_name_df['Rep Name']]\n",
    "\n",
    "# define new columns that represent the data we will use to group the data for\n",
    "# don't want to change original columns etc so we don't have to change data\n",
    "CD_merged_mind[\n",
    "    ['Group Name', 'Group Town', 'Group County', 'Group State', 'Group Country', 'Group Name Type', 'Group Match Index',\n",
    "     'Group Match Status', 'Group Match Url']] = CD_merged_mind[\n",
    "    ['Full Search Name', 'new_town', 'county', 'new_state', 'country', 'name_type', 'Match Index', 'Match Status',\n",
    "     'url']]\n",
    "\n",
    "# iterate through names\n",
    "for ind in group_name_df.index:\n",
    "    # obtain data - list of names and indices that we're replacing and the indexes of the data we are changing, and the indices where we're getting the data we're changing it to from\n",
    "    match_ind, full_name, rep_name = group_name_df.loc[ind, ['Match Index', 'Full Search Name', 'Rep Name']]\n",
    "    change_ind = CD_merged_mind[\n",
    "        CD_merged_mind.apply(lambda x: x['Match Index'] == match_ind and x['Full Search Name'] == full_name,\n",
    "                             axis=1)].index\n",
    "    info_ind = CD_merged_mind[\n",
    "        CD_merged_mind.apply(lambda x: x['Match Index'] == match_ind and x['Full Search Name'] == rep_name,\n",
    "                             axis=1)].index\n",
    "\n",
    "    # sometimes the particular rep name has multiple types of locations - we want to pick the one that is most specific, so town > county > state\n",
    "    # we will unify these locations later\n",
    "    if CD_merged_mind.loc[info_ind, ['Group Name', 'Group Town', 'Group County', 'Group State', 'Group Country',\n",
    "                                     'Group Name Type']].drop_duplicates().shape[0] > 1:\n",
    "        # possible data we are replacing with\n",
    "        possibilities = CD_merged_mind.loc[\n",
    "            info_ind, ['Group Name', 'Group Town', 'Group County', 'Group State', 'Group Country', 'Group Name Type',\n",
    "                       'Group Match Index', 'Group Match Status', 'Group Match Url']].drop_duplicates()\n",
    "        if possibilities[possibilities['Group Name Type'] == 'town'].shape[0] == 1:\n",
    "            possibilities = possibilities[possibilities['Group Name Type'] == 'town'].values.tolist()\n",
    "        elif possibilities[possibilities['Group Name Type'] == 'county'].shape[0] == 1:\n",
    "            possibilities = possibilities[possibilities['Group Name Type'] == 'county'].values.tolist()\n",
    "        elif possibilities[possibilities['Group Name Type'] == 'state'].shape[\n",
    "            0] == 1 or rep_name == 'Benjamin Tallmadge':\n",
    "            possibilities = possibilities[possibilities['Group Name Type'] == 'state'].values.tolist()\n",
    "            if rep_name == 'Benjamin Tallmadge':\n",
    "                possibilities = [ele for ele in possibilities if ele[3] == 'CT']\n",
    "        else:\n",
    "            possibilities = possibilities[possibilities['Group Name Type'] == 'country'].values.tolist()\n",
    "    # if there's only one type of location, no selection process needed\n",
    "    else:\n",
    "        possibilities = CD_merged_mind.loc[\n",
    "            info_ind, ['Group Name', 'Group Town', 'Group County', 'Group State', 'Group Country', 'Group Name Type',\n",
    "                       'Group Match Index', 'Group Match Status', 'Group Match Url']].drop_duplicates().values.tolist()\n",
    "    # make sure we're only assigning one row of data\n",
    "    assert (len(possibilities) == 1)\n",
    "    CD_merged_mind.loc[\n",
    "        change_ind, ['Group Name', 'Group Town', 'Group County', 'Group State', 'Group Country', 'Group Name Type',\n",
    "                     'Group Match Index', 'Group Match Status', 'Group Match Url']] = possibilities[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# import names that we want to group together\n",
    "# these might not be names that have the same match index, or names that have match index == \"\"\n",
    "# contains location because we can only identify a person uniquely through name + location\n",
    "rep_names = pd.read_csv('clean_tools/name_agg.csv')\n",
    "rep_names['original'] = rep_names['original'].apply(lambda x: x.replace(\"\\t\", \" \"))\n",
    "rep_names['new'] = rep_names['new'].apply(lambda x: x.replace(\"\\t\", \" \"))\n",
    "rep_names['location'] = rep_names['location'].apply(lambda x: x.replace(\"\\t\", \" \") if not pd.isnull(x) else x)\n",
    "# notes - we identified the names using fuzzy matching, and then picked the name that was \"right\" by finding whether it was matched, or using ancestry.com to see which one had the most records that seemed correct\n",
    "# this was quite an involved process - below commented code demonstrates how you would iterate through examining the options"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\nny_names = CD_merged_mind[CD_merged_mind['state'] == 'NY']['Group Name'].unique()\\nny_names = [n for n in ny_names if not pd.isnull(n)]\\nmatch_names = [process.extract(name, ny_names, score_cutoff = 80) for name in ny_names]\\nmatch_names_fin = [m for m in match_names if len(m) != 1]\\ni = -1\\n\""
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ny_names = CD_merged_mind[CD_merged_mind['state'] == 'NY']['Group Name'].unique()\n",
    "ny_names = [n for n in ny_names if not pd.isnull(n)]\n",
    "match_names = [process.extract(name, ny_names, score_cutoff = 80) for name in ny_names]\n",
    "match_names_fin = [m for m in match_names if len(m) != 1]\n",
    "i = -1\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\ni+=1\\nname_options = [n[0] for n in match_names_fin[i]]\\nCD_merged_mind[CD_merged_mind.apply(lambda x: x['Group State'] == 'NY' and x['Group Name'] in name_options, axis = 1)][[#'Name',\\n    'Group Name', 'Group State', 'Group Name Type', 'Group Match Index', 'Group Match Status', 'Group Match Url']].drop_duplicates()\\n\""
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "i+=1\n",
    "name_options = [n[0] for n in match_names_fin[i]]\n",
    "CD_merged_mind[CD_merged_mind.apply(lambda x: x['Group State'] == 'NY' and x['Group Name'] in name_options, axis = 1)][[#'Name',\n",
    "    'Group Name', 'Group State', 'Group Name Type', 'Group Match Index', 'Group Match Status', 'Group Match Url']].drop_duplicates()\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# we want to replace the location/info of the original name with the info of the new name\n",
    "for og_name, new_name, loc in zip(rep_names['original'], rep_names['new'], rep_names['location']):\n",
    "    # obtain values that we want to set the person's information to\n",
    "    if pd.isnull(loc):\n",
    "        vals = CD_merged_mind[CD_merged_mind['Group Name'] == new_name][\n",
    "            ['Group Name', 'Group Town', 'Group County', 'Group State', 'Group Country', 'Group Name Type',\n",
    "             'Group Match Index', 'Group Match Status', 'Group Match Url']].drop_duplicates().values.tolist()\n",
    "    else:\n",
    "        town, county, state, nametype = parseLocationString(loc, loc.split(\" | \")[-1])\n",
    "        vals = CD_merged_mind[CD_merged_mind.apply(\n",
    "            lambda x: x['Group Name'] == new_name and x['Group Town'] == town and x['Group County'] == county and x[\n",
    "                'Group State'] == state and x['Group Name Type'] == nametype, axis=1)][\n",
    "            ['Group Name', 'Group Town', 'Group County', 'Group State', 'Group Country', 'Group Name Type',\n",
    "             'Group Match Index', 'Group Match Status', 'Group Match Url']].drop_duplicates().values.tolist()\n",
    "    if new_name == 'Henry Huttenstein':\n",
    "        vals = [ele for ele in vals if ele[1] == 'Lancaster']\n",
    "\n",
    "    # obtain index of the name whose information we want to replace\n",
    "    # two cases - for if we need to input location information or not\n",
    "    if CD_merged_mind[CD_merged_mind.apply(lambda x: x['Group Name'] == og_name, axis=1)][\n",
    "        ['Group Town', 'Group County', 'Group State', 'Group Country', 'Group Name Type']].drop_duplicates().shape[\n",
    "        0] > 1:\n",
    "        # special exception\n",
    "        if og_name == 'Benjamin Brown' or og_name == 'William Wheater':\n",
    "            rep_ind = CD_merged_mind[\n",
    "                CD_merged_mind.apply(lambda x: x['Group Name'] == og_name and x['Group State'] == 'RI', axis=1)].index\n",
    "        else:\n",
    "            rep_ind = CD_merged_mind[CD_merged_mind.apply(\n",
    "                lambda x: x['Group Name'] == og_name and x['Group Town'] == town and x['Group County'] == county and x[\n",
    "                    'Group State'] == state and x['Group Name Type'] == nametype, axis=1)].index\n",
    "    else:\n",
    "        rep_ind = CD_merged_mind[CD_merged_mind.apply(lambda x: x['Group Name'] == og_name, axis=1)].index\n",
    "\n",
    "    # make sure we're only assigning one row of data - otherwise we have issues\n",
    "    if len(vals) == 1:\n",
    "        CD_merged_mind.loc[\n",
    "            rep_ind, ['Group Name', 'Group Town', 'Group County', 'Group State', 'Group Country', 'Group Name Type',\n",
    "                      'Group Match Index', 'Group Match Status', 'Group Match Url']] = vals[0]\n",
    "    if len(vals) > 1:\n",
    "        print(og_name, new_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# now, there are some names that are the same, but have different locations. We think that we can identify these by finding cases where the name is the same, but the locations differ (up to the state level)\n",
    "# example: Man in montgomery MD, Man in MD\n",
    "dup_state = CD_merged_mind[\n",
    "    ['Group Name', 'Group State', 'Group County', 'Group Town', 'Group Name Type']].drop_duplicates().groupby(\n",
    "    ['Group Name', 'Group State']).count().reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "for ind in dup_state[dup_state['Group County'] > 1].index:\n",
    "    # information from dataframe with potentially duplicated individuals (people with same name, identity but different locations)\n",
    "    name, state = dup_state.loc[ind, ['Group Name', 'Group State']]\n",
    "\n",
    "    # values of all possible name types\n",
    "    vals =  CD_merged_mind[CD_merged_mind.apply(lambda x: x['Group Name'] == name and x['Group State'] == state, axis=1)][\n",
    "        'Group Name Type'].drop_duplicates().tolist()\n",
    "    # list of unique towns\n",
    "    towns = [ele for ele in CD_merged_mind[\n",
    "        CD_merged_mind.apply(lambda x: x['Group Name'] == name and x['Group State'] == state, axis=1)][\n",
    "        'Group Town'].drop_duplicates().tolist() if ele != \"\"]\n",
    "    # list of unique counties\n",
    "    counties = [ele for ele in CD_merged_mind[\n",
    "        CD_merged_mind.apply(lambda x: x['Group Name'] == name and x['Group State'] == state, axis=1)][\n",
    "        'Group County'].drop_duplicates().tolist() if ele != \"\"]\n",
    "\n",
    "    # let towns, counties length equal 1 (excluding \"\") because we don't want contradicting information. for example, montgomery MD and anne arundel MD contradict but annapolis, anne arundel MD and anne arundel MD don't contradict because one is more specific than the other\n",
    "    if len(vals) > 1 and dup_state.loc[ind, 'Group County'] < 3 and len(towns) == 1 and len(counties) == 1:\n",
    "        # find what is the most specific location we have\n",
    "        if 'town' in vals:\n",
    "            change_val = CD_merged_mind.loc[CD_merged_mind.apply(\n",
    "                lambda x: x['Group Name'] == name and x['Group State'] == state and x['Group Name Type'] == 'town',\n",
    "                axis=1), ['Group Name', 'Group Town', 'Group County', 'Group State', 'Group Name Type',\n",
    "                          'Group Match Index', 'Group Match Status', 'Group Match Url']].drop_duplicates().values\n",
    "        elif 'county' in vals:\n",
    "            change_val = CD_merged_mind.loc[CD_merged_mind.apply(\n",
    "                lambda x: x['Group Name'] == name and x['Group State'] == state and x['Group Name Type'] == 'county',\n",
    "                axis=1), ['Group Name', 'Group Town', 'Group County', 'Group State', 'Group Name Type',\n",
    "                          'Group Match Index', 'Group Match Status', 'Group Match Url']].drop_duplicates().values\n",
    "        else:  # 'state' in vals\n",
    "            change_val = CD_merged_mind.loc[CD_merged_mind.apply(\n",
    "                lambda x: x['Group Name'] == name and x['Group State'] == state and x['Group Name Type'] == 'state',\n",
    "                axis=1), ['Group Name', 'Group Town', 'Group County', 'Group State', 'Group Name Type',\n",
    "                          'Group Match Index', 'Group Match Status', 'Group Match Url']].drop_duplicates().values\n",
    "\n",
    "        change_ind = CD_merged_mind[\n",
    "            CD_merged_mind.apply(lambda x: x['Group Name'] == name and x['Group State'] == state, axis=1)].index\n",
    "        CD_merged_mind.loc[change_ind, ['Group Name', 'Group Town', 'Group County', 'Group State', 'Group Name Type',\n",
    "                                        'Group Match Index', 'Group Match Status', 'Group Match Url']] = change_val[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# define some columns that we will use later when grouping data - makes tracking stuff easier\n",
    "CD_merged_mind['data_index'] = CD_merged_mind['state_data'] + \"_\" + CD_merged_mind['state_data_index'].astype(str)\n",
    "CD_merged_mind['assets'] = CD_merged_mind['data_index'] + \" : \" + CD_merged_mind['6p_total'].astype(str) + \", \" + CD_merged_mind['6p_def_total'].astype(str) + \", \" + CD_merged_mind['3p_total'].astype(str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# fill in information for people who we could not search for on ancestry because their name was not formatted as a name - these are the unsearchables from the third cleaning script\n",
    "na_ind = CD_merged_mind[CD_merged_mind['Group Name'].isnull()].index\n",
    "CD_merged_mind.loc[na_ind, 'Group Name'] = CD_merged_mind.loc[na_ind, 'Name']\n",
    "CD_merged_mind.loc[na_ind, 'Full Search Name'] = CD_merged_mind.loc[na_ind, 'Name']\n",
    "CD_merged_mind.loc[na_ind, 'Group Match Index'] = 'Unsearchable (not a name)'\n",
    "CD_merged_mind.loc[na_ind, 'Group Match Url'] = 'Unsearchable (not a name)'\n",
    "CD_merged_mind.loc[na_ind, 'Name_Fix'] = CD_merged_mind.loc[na_ind, 'Name']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# capitalize nams correctly\n",
    "CD_merged_mind['Group Name'] = CD_merged_mind['Group Name'].apply(\n",
    "    lambda x: \" \".join([ele.capitalize() if \"ii\" not in ele.lower() else ele.upper() for ele in x.split(\" \")]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# manually fix an entry - i don't know why this was not addressed by the above, but now it is fixed\n",
    "CD_merged_mind.loc[\n",
    "    CD_merged_mind[CD_merged_mind['Name'].apply(lambda x: 'Bowdle' in x)].index, 'Group Town'] = 'Annapolis'\n",
    "CD_merged_mind.loc[\n",
    "    CD_merged_mind[CD_merged_mind['Name'].apply(lambda x: 'Gassaway Watkins' in x)].index, 'Group Town'] = 'Annapolis'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# group together everyone with the same name, location and aggregate their assets and information\n",
    "df_final = CD_merged_mind.fillna(\"\").groupby(\n",
    "    ['Group Name', 'Group State', 'Group County', 'Group Town', 'Group Name Type', 'Group Match Index',\n",
    "     'Group Match Url']).agg({'Name_Fix': lambda x: list(set(x)), 'Full Search Name': tNameList, 'assets': tNameList,\n",
    "                              'occupation': tNameList}).reset_index()\n",
    "exception_names = []\n",
    "\n",
    "# find cases where we have the same name but in different locations - we think that these are actually the same people so we want to group them together\n",
    "dup_state_2 = df_final.explode(\"Name_Fix\")[\n",
    "    ['Name_Fix', 'Group Name', 'Group State', 'Group County', 'Group Town', 'Group Name Type']].drop_duplicates()\n",
    "dup_state_2['Name_Fix'] = dup_state_2['Name_Fix'].apply(lambda x: x.split(\" | \"))\n",
    "dup_state_2 = dup_state_2.explode('Name_Fix').drop_duplicates().groupby(\n",
    "    ['Name_Fix', 'Group State']).nunique().reset_index()\n",
    "\n",
    "# basically, we want to keep looping through this until the number of exceptions is the same as the number of ungrouped names\n",
    "# exception names are names we can't merge, there's a process by which we can't combine them (basically if they have contradicting info - so Bob is in two different counties, then Bob is an exception)\n",
    "# we have to run the loop multiple times because sometimes a name can be changed multiple times after its grouped\n",
    "while len(exception_names) != dup_state_2[dup_state_2.apply(lambda x: x['Group County'] > 1 and x['Group Name'] > 1, axis=1)].shape[0]:\n",
    "    dup_state_2 = df_final.explode(\"Name_Fix\")[\n",
    "        ['Name_Fix', 'Group Name', 'Group State', 'Group County', 'Group Town', 'Group Name Type']].drop_duplicates()\n",
    "    dup_state_2['Name_Fix'] = dup_state_2['Name_Fix'].apply(lambda x: x.split(\" | \"))\n",
    "    dup_state_2 = dup_state_2.explode('Name_Fix').drop_duplicates().groupby(\n",
    "        ['Name_Fix', 'Group State']).nunique().reset_index()\n",
    "\n",
    "    for ind in dup_state_2[dup_state_2.apply(lambda x: x['Group County'] > 1 and x['Group Name'] > 1, axis=1)].index:\n",
    "        # information from dataframe with potentially duplicated individuals (people with same name, identity but different locations)\n",
    "        name, state = dup_state_2.loc[ind, ['Name_Fix', 'Group State']]\n",
    "\n",
    "        # values of all possible name types\n",
    "        vals = df_final[\n",
    "            df_final.apply(lambda x: any([name in ele for ele in x['Name_Fix']]) and x['Group State'] == state,\n",
    "                           axis=1)]['Group Name Type'].drop_duplicates().tolist()\n",
    "        # list of unique towns\n",
    "        towns = [ele for ele in df_final[\n",
    "            df_final.apply(lambda x: any([name in ele for ele in x['Name_Fix']]) and x['Group State'] == state,\n",
    "                           axis=1)]['Group Town'].drop_duplicates().tolist() if ele != \"\"]\n",
    "        # list of unique counties\n",
    "        counties = [ele for ele in df_final[\n",
    "            df_final.apply(lambda x: any([name in ele for ele in x['Name_Fix']]) and x['Group State'] == state,\n",
    "                           axis=1)]['Group County'].drop_duplicates().tolist() if ele != \"\"]\n",
    "\n",
    "        # let towns, counties length equal 1 (excluding \"\") because we don't want contradicting information. for example, montgomery MD and anne arundel MD contradict but annapolis, anne arundel MD and anne arundel MD don't contradict because one is more specific than the other\n",
    "        if len(towns) <= 1 and len(counties) == 1:\n",
    "            # find\n",
    "            if 'town' in vals:\n",
    "                change_val = df_final.loc[df_final.apply(\n",
    "                    lambda x: any([name in ele for ele in x['Name_Fix']]) and x['Group State'] == state and x[\n",
    "                        'Group Name Type'] == 'town', axis=1), ['Group Town', 'Group County', 'Group State',\n",
    "                                                                'Group Name Type']].drop_duplicates().values\n",
    "            elif 'county' in vals:\n",
    "                change_val = df_final.loc[df_final.apply(\n",
    "                    lambda x: any([name in ele for ele in x['Name_Fix']]) and x['Group State'] == state and x[\n",
    "                        'Group Name Type'] == 'county', axis=1), ['Group Town', 'Group County', 'Group State',\n",
    "                                                                  'Group Name Type']].drop_duplicates().values\n",
    "            else:  # 'state' in vals\n",
    "                change_val = df_final.loc[df_final.apply(\n",
    "                    lambda x: any([name in ele for ele in x['Name_Fix']]) and x['Group State'] == state and x[\n",
    "                        'Group Name Type'] == 'state', axis=1), ['Group Town', 'Group County', 'Group State',\n",
    "                                                                 'Group Name Type']].drop_duplicates().values\n",
    "\n",
    "            change_ind = df_final[\n",
    "                df_final.apply(lambda x: any([name in ele for ele in x['Name_Fix']]) and x['Group State'] == state,\n",
    "                               axis=1)].index\n",
    "            assert (len(change_val) == 1)\n",
    "            df_final.loc[change_ind, ['Group Town', 'Group County', 'Group State', 'Group Name Type']] = change_val[0]\n",
    "        else:\n",
    "            if [name, state] not in exception_names:\n",
    "                exception_names.append([name, state])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5s/dvrxt95949x1pm_sjxm85lj00000gn/T/ipykernel_51442/2420180601.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  other_names['Full Search Name'] = other_names['Full Search Name'].apply(lambda x: x.split(\" | \"))\n"
     ]
    }
   ],
   "source": [
    "# now, we want to find a way to systematize all the names - for example, Bob Rush and Bob Rushe are the same person but in our notation they are denoted as separate people\n",
    "# we will use a dictionary to convert all Bob Rushe's in a particular location into Bob Rush\n",
    "other_names = df_final[['Group Name', 'Group State', 'Group County', 'Group Town', 'Full Search Name']]\n",
    "other_names['Full Search Name'] = other_names['Full Search Name'].apply(lambda x: x.split(\" | \"))\n",
    "other_names = other_names.explode('Full Search Name')\n",
    "namechange_dict = dict(zip(\n",
    "    other_names['Full Search Name'] + other_names['Group Town'] + other_names['Group County'] + other_names[\n",
    "        'Group State'], other_names['Group Name']))\n",
    "# some manual additions due to idosyncracies of cleaning process\n",
    "namechange_dict['DesdeilyNY'] = 'Desdeily'\n",
    "namechange_dict['GrundNY'] = 'Grund'\n",
    "namechange_dict['Thomas Cloyd HalseyProvidenceProvidence CountyRI'] = 'Thomas Lloyd Halsey'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# more names that we add to the dictionary manually - we have to do this for the exceptions because these people are not accounted for for some weird reason in the original process of creating the dictionary\n",
    "# also a special exception for how we process Samuel Vernon's name\n",
    "for ele in exception_names:\n",
    "    name, state = ele[0], ele[1]\n",
    "    vals = df_final[\n",
    "        df_final.apply(lambda x: any([name in ele for ele in x['Name_Fix']]) and x['Group State'] == state, axis=1)][\n",
    "        ['Group Town', 'Group County', 'Group State']].drop_duplicates()\n",
    "    for ind in vals.index:\n",
    "        town, county, state = vals.loc[ind, 'Group Town'], vals.loc[ind, 'Group County'], vals.loc[ind, 'Group State']\n",
    "        if name + town + county + state not in namechange_dict.keys():\n",
    "            reps = [e for e in namechange_dict.keys() if name in e]\n",
    "            if name == 'Samuel Vernon':\n",
    "                namechange_dict[name + town + county + state] = name\n",
    "            else:\n",
    "                assert (len(list(set([namechange_dict[ele] for ele in reps]))) == 1)\n",
    "                namechange_dict[name + town + county + state] = namechange_dict[reps[0]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# now we reformat the name\n",
    "# : separates different institutions associated with someone, so for example, someone might own bonds as an individual Bob and with his friends (Bob and George) so the result is Bob : Bob | George\n",
    "# ASDSD helps us find places where our dictionary fails\n",
    "df_final['Name_Fix_Transfer'] = df_final.apply(lambda x: \" : \".join(list(set([(tNameList([namechange_dict.get(\n",
    "    subele + x['Group Town'] + x['Group County'] + x['Group State'],\n",
    "    \"ASDSD\" + subele + x['Group Town'] + x['Group County'] + x['Group State']) for subele in ele.split(\" | \")]) + \" / \" + ele) for ele\n",
    "    in x['Name_Fix']]))), axis=1)\n",
    "df_final['Name_Fix_Clean'] = df_final.apply(lambda x: \" : \".join(list(set([tNameList([namechange_dict.get(\n",
    "    subele + x['Group Town'] + x['Group County'] + x['Group State'],\n",
    "    \"ASDSD\" + subele + x['Group Town'] + x['Group County'] + x['Group State']) for subele in ele.split(\" | \")])  for ele\n",
    "                                                                              in x['Name_Fix']]))), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "df_final.drop('Name_Fix', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "['John Gale', 'Love Stone', 'Nathaniel Irwin', 'Tristram Bowdle']"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(\n",
    "    df_final[df_final.duplicated(['Group Name', 'Group State', 'Group County', 'Group Town', 'Group Name Type'])][\n",
    "        'Group Name'].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# now, we have a case where the same person and location appears multiple times, so we need to combine them into one person. we do that here\n",
    "# above we confirm the people for who we have to do this for\n",
    "df_final.loc[\n",
    "    df_final[df_final['Group Name'] == 'Love Stone'].index, ['Group Match Url', 'Name_Fix_Clean', 'Name_Fix_Transfer',\n",
    "                                                             'assets', 'occupation']] = [\n",
    "    'https://www.ancestrylibrary.com/search/collections/5058/?name=Love_Stone&name_x=1_1&residence=_charleston-south carolina-usa_552&residence_x=_1-0',\n",
    "    'Joseph Vesey | Love Stone : Love Stone',\n",
    "    'Joseph Vesey | Love Stone / Joseph Vesey | Love Stone : Love Stone / Love Stone',\n",
    "    'SC_10 : 216.67, 108.33, 207.64 | SC_394 : 881.66, 2838.33, 856.64',\n",
    "    'Administrators of Joseph Darrell | Widow']\n",
    "\n",
    "df_final.loc[\n",
    "    df_final[df_final['Group Name'] == 'John Gale'].index, ['Group Match Url', 'Group Match Index', 'Name_Fix_Clean',\n",
    "                                                            'Name_Fix_Transfer',\n",
    "                                                            'assets', 'occupation']] = [\n",
    "    'https://www.ancestrylibrary.com/search/collections/5058/?name=John_Gale&name_x=1_1&residence=_Maryland-usa_23&residence_x=_1-0',\n",
    "    '984 | 985', 'Ebenezer Finlays | John Gale : John Gale',\n",
    "    'John Gale / John Gale : Ebenezer Finlays | John Gale / Ebenezer Finlays | John Gale',\n",
    "    'MD_243 : 2148.66, 1074.34, 1409.32 | MD_244 : 1036.86, 518.43, 683.74 | PA_1080 : 176.58, 88.29, 201.48',\n",
    "    ' | Executor of Ebenezer Finlay']\n",
    "\n",
    "df_final.loc[\n",
    "    df_final[df_final['Group Name'] == 'Nathaniel Irwin'].index, ['Group Match Index', 'Group Match Url',\n",
    "                                                                  'Name_Fix_Clean', 'Name_Fix_Transfer',\n",
    "                                                                                    'assets', 'occupation']] = ['1988',\n",
    "                                                                                                                'https://www.ancestrylibrary.com/search/collections/5058/?name=Nathaniel_Irwin&name_x=s_s&residence=_bucks-pennsylvania-usa_403&residence_x=_1-0',\n",
    "                                                                                                                'Nathaniel Irwin : Nathaniel Irwin | Richard Walker',\n",
    "                                                                                                                'Nathaniel Irwin | Richard Walker / Nathaniel Irwin | Richard Walker : Nathaniel Irwin / Nathaniel Irwin',\n",
    "                                                                                                                'PA_693 : 276.68, 138.35, 93.77 | PA_1117 : 0.0, 0.0, 26.02 | PA_949 : 617.36, 308.69, 179.16',\n",
    "                                                                                                                'Administer Estate of Richard Walker Deceased']\n",
    "\n",
    "df_final.loc[\n",
    "    df_final[df_final['Group Name'] == 'Moses Brown'].index, ['Group Match Index', 'Group Match Url', 'Name_Fix_Clean',\n",
    "                                                              'Name_Fix_Transfer',\n",
    "                                                              'assets', 'occupation']] = ['2472',\n",
    "                                                                                          'https://www.ancestrylibrary.com/search/collections/5058/?name=Moses_Brown&name_x=1_1&residence=_providence-providence-rhode island-usa_5531&residence_x=_1-0',\n",
    "                                                                                          'Moses Brown | Nicholas Brown : John Francis | Moses Brown : Moses Brown',\n",
    "                                                                                          'Moses Brown | Nicholas Brown / Mess Brown | Moses Brown | Nicholas Brown : Moses Brown / Moses Brown : Moses Brown | Nicholas Brown / Moses Brown | Nicholas Brown : John Francis | Moses Brown / John Francis | MOses Brown : Moses Brown / Moses Brown',\n",
    "                                                                                          'RI_126 : 568.96, 284.47, 242.46 | RI_285 : 480.0, 240.0, 24095.11 | RI_334 : 266.67, 133.33, 72.0 | RI_516 : 942.65, 7889.63, 9392.12 | RI_598 : 4675.35, 1805.23, 2092.76 | RI_604 : 18067.23, 9033.61, 4878.15',\n",
    "                                                                                          'Esquire | Extors to the Late Nicholas Brown Esq Deceased']\n",
    "\n",
    "df_final.loc[df_final[df_final['Group Name'].apply(lambda x: 'Peleg San' in x)].index, ['Group Name', 'Group State',\n",
    "                                                                                        'Group County', 'Group Town',\n",
    "                                                                                        'Group Name Type',\n",
    "                                                                                        'Group Match Index',\n",
    "                                                                                        'Group Match Url',\n",
    "                                                                                        'Name_Fix_Clean',\n",
    "                                                                                        'Name_Fix_Transfer',\n",
    "                                                                                        'Full Search Name', 'assets',\n",
    "                                                                                        'occupation']] = [\n",
    "    'Peleg Sanford', 'CT', 'Hartford County', 'Hartford', 'town', '13',\n",
    "    'https://www.ancestrylibrary.com/search/collections/5058/?name=Peleg_Sanford&name_x=ps_ps&residence=_hartford-hartford-connecticut-usa_999&residence_x=_1-1-a',\n",
    "    'Peleg Sanford', 'Peleg Sandford : Peleg Sandford / Peleg Sanford : Peleg Sanford / Peleg Sanford', 'Peleg Sanford | Peleg Sandford',\n",
    "    'CT_13 : 1000.17, 500.09, 1500.32 | CT_280 : 506.93, 253.47, 500.15 | CT_308 : 1200.0, 0.0, 0.0 | CT_672 : 389.47, 194.74, 177.76 | CT_836 : 39.82, 19.92, 0.0 | NY_1773 : 0.0, 0.0, 2665.61 | NY_2107 : 0.0, 0.0, 288.0 | NY_1773 : 0.0, 0.0, 2665.61 | NY_2107 : 0.0, 0.0, 288.0',\n",
    "    'Merchant']\n",
    "\n",
    "df_final.loc[\n",
    "    df_final[df_final['Group Name'] == 'Tristram Bowdle'].index, ['Group Match Url', 'Name_Fix_Clean', 'Name_Fix_Transfer', 'Full Search Name',\n",
    "                                                                  'assets']] = [\n",
    "    'https://www.ancestrylibrary.com/search/collections/5058/?name=Tristram_Bowdle&name_x=ps_ps&residence=_anne+arundel-maryland-usa_169&residence_x=_1-1',\n",
    "    'Gassaway Watkins | Tristram Bowdle : Tristram Bowdle',\n",
    "    'Gassaway Watkins | Tristram Bowdle / Gassaway Watkins | Tristiam Bowdle | Tristram Bowdle : Tristram Bowdle / Tristram Bowdle',\n",
    "    'Tristiam Bowdle | Tristram Bowdle',\n",
    "    'MD_590 : 38.93, 62.48, 108.28 | MD_591 : 124.96, 0.0, 0.0']\n",
    "\n",
    "df_final.drop_duplicates(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# next, we have people who are the same, but live in different states (have the same name). these people have different states because earlier when we were cleaning code (in the first notebook), we imputed the state that someone lived in when the state was missing as the state of the debt file but sometimeds, these people are not from that state\n",
    "# list of people\n",
    "state_group_names = pd.read_csv('clean_tools/group_name_state.csv')\n",
    "# in cases where we have people with the same name, and both names only have location information at the state level, we specify here which state we're choosing - basde off preliminary analysis\n",
    "pickstate = {'Samuel W Johnson': 'NY', 'Josiah Watson': 'VA', 'Gerrard Alexander': 'VA', 'Benjamin Tallmadge': 'CT',\n",
    "             'Edward Chinn': 'NY', 'Forman Mount': 'PA', 'Josiah Watson': 'VA', 'Thomas Robinson': 'DE',\n",
    "             'Thomas Ross': 'SC', 'William Applegate': 'NJ'}\n",
    "for ind in state_group_names.index:\n",
    "    # get the name and the index/dataframe associated with the name\n",
    "    group_name = state_group_names.loc[ind, 'Group Name']\n",
    "    df_ind = df_final[df_final['Group Name'] == group_name].index\n",
    "    df_final_sub = df_final.loc[df_ind]\n",
    "    rep_vals = [group_name]\n",
    "    # if the group name type is state for all appearances of that individual then we refer to the pickstate dictionary to get the state we want to keep\n",
    "    if len(df_final.loc[df_ind, 'Group Name Type'].unique()) == 1 and df_final.loc[df_ind, 'Group Name Type'].unique()[\n",
    "        0] == 'state':\n",
    "        rep_vals.extend(df_final_sub[df_final_sub['Group State'].apply(lambda x: x == pickstate[group_name])][\n",
    "                            ['Group State', 'Group County', 'Group Town', 'Group Name Type', 'Group Match Index',\n",
    "                             'Group Match Url']].values.tolist()[0])\n",
    "        for col in ['Full Search Name', 'assets', 'occupation','Name_Fix_Transfer', 'Name_Fix_Clean']:\n",
    "            # have to handle the way we combine the name differently for Name_Fix column\n",
    "            if col != 'Name_Fix_Clean' and col != 'Name_Fix_Transfer':\n",
    "                rep_vals.append(tNameList(\" | \".join(df_final_sub.loc[df_ind, col].tolist()).split(\" | \")))\n",
    "            else:\n",
    "                rep_vals.append(\" : \".join(list(set(\" : \".join(df_final_sub.loc[df_ind, col].tolist()).split(\" : \")))))\n",
    "    else:\n",
    "        # pick the geography level that's most specific\n",
    "        rep_vals.extend(df_final_sub[df_final_sub['Group Name Type'].apply(lambda x: x == 'town' or x == 'county')][\n",
    "                            ['Group State', 'Group County', 'Group Town', 'Group Name Type', 'Group Match Index',\n",
    "                             'Group Match Url']].values.tolist()[0])\n",
    "        for col in ['Full Search Name', 'assets', 'occupation','Name_Fix_Transfer', 'Name_Fix_Clean']:\n",
    "            # have to handle the way we combine the name differently for Name_Fix column\n",
    "            if col != 'Name_Fix_Clean' and col != 'Name_Fix_Transfer':\n",
    "                rep_vals.append(tNameList(\" | \".join(df_final_sub.loc[df_ind, col].tolist()).split(\" | \")))\n",
    "            else:\n",
    "                rep_vals.append(\" : \".join(list(set(\" : \".join(df_final_sub.loc[df_ind, col].tolist()).split(\" : \")))))\n",
    "    # replace the index with the value\n",
    "    df_final.loc[df_ind] = rep_vals\n",
    "\n",
    "# remove people from the location that was removed\n",
    "#df_final.drop_duplicates(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3089    South Carolina\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t...\n",
      "Name: Match State, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# another case where we have to clean the data manually because somehow this entry was not cleaned properly in the third notebook\n",
    "print(match_df.loc[\n",
    "          match_df[match_df['Home in 1790 (City, County, State)'].apply(lambda x: '\\n' in x)].index, 'Match State'])\n",
    "match_df.loc[match_df[match_df['Home in 1790 (City, County, State)'].apply(\n",
    "    lambda x: '\\n' in x)].index, 'Match State'] = 'South Carolina'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# next, we want to define a \"village\" category that tells us what part of Philly/Charleston/New York someone lived in, if it's in their ancestry.com data\n",
    "# we only do this for these 3 towns\n",
    "village_ind = match_df[match_df['Match County'].apply(\n",
    "    lambda x: 'philadelphia' in x.lower() or 'charleston' in x.lower() or 'new york' in x.lower())]['Match Town'].index\n",
    "match_df.loc[village_ind, 'Match Village'] = [ele if ele != 'Philadelphia City' else '' for ele in match_df.loc[village_ind, 'Match Town'] ]\n",
    "match_df.loc[village_ind, 'Match Town'] = [\n",
    "    'Philadelphia' if 'philadelphia' in ele.lower() else 'Charleston' if 'charleston' in ele.lower() else 'New York City'\n",
    "    for ele in match_df.loc[village_ind, 'Match County']]\n",
    "match_df.fillna(\"\", inplace=True)\n",
    "# change match type based on village\n",
    "match_df['Match Type'] = match_df.apply(lambda x: 'village' if x['Match Village'] != '' else x['Match Type'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# next, we're going to clean the match dataframe containing results from ancestry.com\n",
    "# the first way we identify an entry that needs to be cleaned is if it has parentheses but no commas - the occupation is inside the parentheses\n",
    "opt_one_ind = match_df[match_df['Name'].apply(lambda x: '(' in x and ',' not in x)].index\n",
    "# we extract the occupation within the name, add it to an occupation column we create and then remove the occupation from the name\n",
    "match_df.loc[opt_one_ind, 'Occupation'] = match_df.loc[opt_one_ind, 'Name'].apply(\n",
    "    lambda x: tNameList([ele[ele.find(\"(\") + 1:ele.find(\")\")] for ele in x.split(\" | \") if '(' in ele]))\n",
    "match_df.loc[opt_one_ind, 'Name'] = match_df.loc[opt_one_ind].apply(lambda x: tNameList(\n",
    "    [(ele[0:ele.find(\"(\") - 1] + ele[ele.find(\")\") + 1:]) if '(' in ele else ele.replace(x['Occupation'], '') for ele in\n",
    "     x['Name'].split(\" | \")]), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# next, we have a few particular occupations that are noted with a comma in between names\n",
    "esq_ind = match_df[\n",
    "    match_df['Name'].apply(lambda x: ',' in x and '(' not in x and not any(char.isdigit() for char in x))].index\n",
    "# there are only judges esquires or colonels, so we check and add these to the occupations columns if they're there\n",
    "match_df.loc[esq_ind, 'Occupation'] = match_df.loc[esq_ind, 'Name'].apply(lambda x: 'Esquire' if 'Esq' in x else '')\n",
    "match_df.loc[esq_ind, 'Occupation'] = match_df.loc[esq_ind].apply(\n",
    "    lambda x: tNameList((x['Occupation'] + \" | Colonel\").split(\" | \")) if 'Col' in x['Name'] else x['Occupation'],\n",
    "    axis=1)\n",
    "match_df.loc[esq_ind, 'Occupation'] = match_df.loc[esq_ind].apply(\n",
    "    lambda x: tNameList((x['Occupation'] + \" | Judge\").split(\" | \")) if 'Exce' in x['Name'] or 'Judge' in x['Name'] else\n",
    "    x['Occupation'], axis=1)\n",
    "# remove the parts of the name that contain the occupations\n",
    "match_df.loc[esq_ind, 'Name'] = match_df.loc[esq_ind].apply(lambda x: tNameList([ele.replace(',', '').replace('Esquire',\n",
    "                                                                                                              '').replace(\n",
    "    'Esqr', '').replace('Esq.', '').replace('Esq', '').replace('Colonel', '').replace('Col', '').replace('  ',\n",
    "                                                                                                         ' ').replace(\n",
    "    '.', '').replace('His Excely ', '').replace('r|', 'r |').replace('y|', 'y |').replace('n|', 'n |').strip() for ele\n",
    "                                                                                 in x['Name'].split(\" | \")]), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# finally, we have a few particular occupations that are noted with a comma and parentheses in between names\n",
    "both_ind = match_df[match_df['Name'].apply(lambda x: '(' in x and ',' in x)]['Name'].index\n",
    "# first we remove the occupation inside the parentheses and remove it from the name\n",
    "match_df.loc[both_ind, 'Occupation'] = match_df.loc[both_ind, 'Name'].apply(\n",
    "    lambda x: tNameList([ele[ele.find(\"(\") + 1:ele.find(\")\")] for ele in x.split(\" | \") if '(' in ele]))\n",
    "match_df.loc[both_ind, 'Name'] = match_df.loc[both_ind].apply(lambda x: tNameList(\n",
    "    [(ele[0:ele.find(\"(\") - 1] + ele[ele.find(\")\") + 1:]) if '(' in ele else ele.replace(x['Occupation'], '') for ele in\n",
    "     x['Name'].split(\" | \")]), axis=1)\n",
    "# next, the only occupations that use the comma are esquire,colonel and judge so we just follow the same steps as before\n",
    "match_df.loc[both_ind, 'Occupation'] = match_df.loc[both_ind].apply(\n",
    "    lambda x: tNameList((x['Occupation'] + \" | Esquire\").split(\" | \")) if 'Esq' in x['Name'] else x['Occupation'],\n",
    "    axis=1)\n",
    "match_df.loc[both_ind, 'Occupation'] = match_df.loc[both_ind].apply(\n",
    "    lambda x: tNameList((x['Occupation'] + \" | Colonel\").split(\" | \")) if 'Col' in x['Name'] else x['Occupation'],\n",
    "    axis=1)\n",
    "match_df.loc[both_ind, 'Occupation'] = match_df.loc[both_ind].apply(\n",
    "    lambda x: tNameList((x['Occupation'] + \" | Judge\").split(\" | \")) if 'Exce' in x['Name'] or 'Judge' in x['Name'] else\n",
    "    x['Occupation'], axis=1)\n",
    "# remove those occupations from the name\n",
    "match_df.loc[both_ind, 'Name'] = match_df.loc[both_ind].apply(lambda x: tNameList([ele.replace(',', '').replace(\n",
    "    'Esquire', '').replace('Esqr', '').replace('Esq.', '').replace('Esq', '').replace('Colonel', '').replace('Col',\n",
    "                                                                                                             '').replace(\n",
    "    '  ', ' ').replace('.', '').replace('His Excely ', '').replace('r|', 'r |').replace('y|', 'y |').replace('n|',\n",
    "                                                                                                             'n |').strip()\n",
    "                                                                                   for ele in x['Name'].split(\" | \")]),\n",
    "                                                              axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# Finally, we have a few occupations - judges, reverends, majors, doctors and colonels that are not noted with anything - they are just aprt of the name, so we go through these one by one, add the occupation and remove the occupation names from the actual name column\n",
    "# judges\n",
    "honor_index = match_df[match_df['Name'].apply(lambda x: 'Honr.' in x or 'Honorable' in x or 'Honererable' in x)].index\n",
    "match_df.loc[honor_index, 'Occupation'] = match_df.fillna(\"\").loc[honor_index, 'Occupation'].apply(\n",
    "    lambda x: tNameList((x + \" | Judge\").split(\" | \")))\n",
    "match_df.loc[honor_index, 'Name'] = match_df.loc[honor_index].apply(lambda x: tNameList(\n",
    "    [ele.replace(\"Honr.\", \"\").replace(\"Honorable\", \"\").replace(\"Honererable\", \"\").replace(\"  \", \" \").strip() for ele in\n",
    "     x['Name'].split(\" | \")]), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# reverends\n",
    "rev_index = match_df[match_df['Name'].apply(lambda x: 'Revd' in x or 'Reverend' in x)].index\n",
    "match_df.loc[rev_index, 'Occupation'] = match_df.fillna(\"\").loc[rev_index, 'Occupation'].apply(\n",
    "    lambda x: tNameList((x + \" | Reverend\").split(\" | \")))\n",
    "match_df.loc[rev_index, 'Name'] = match_df.loc[rev_index].apply(lambda x: tNameList(\n",
    "    [ele.replace(\"Revd\", \"\").replace(\"Reverend\", \"\").replace(\"  \", \" \").strip() for ele in x['Name'].split(\" | \")]),\n",
    "                                                                axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# majors\n",
    "maj_index = match_df[match_df['Name'].apply(lambda x: 'Majr' in x or 'Major' in x)].index\n",
    "match_df.loc[maj_index, 'Occupation'] = match_df.fillna(\"\").loc[maj_index, 'Occupation'].apply(\n",
    "    lambda x: tNameList((x + \" | Major\").split(\" | \")))\n",
    "match_df.loc[maj_index, 'Name'] = match_df.loc[maj_index].apply(lambda x: tNameList(\n",
    "    [ele.replace(\"Majr\", \"\").replace(\"Major\", \"\").replace(\"  \", \" \").strip() for ele in x['Name'].split(\" | \")]),\n",
    "                                                                axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# doctors\n",
    "doctor_index = match_df[\n",
    "    match_df['Name'].apply(lambda x: 'Dr ' in x or 'Doctor' in x or 'Docr' in x or 'Doctr' in x or 'Dortoe' in x)].index\n",
    "match_df.loc[doctor_index, 'Occupation'] = match_df.fillna(\"\").loc[doctor_index, 'Occupation'].apply(\n",
    "    lambda x: tNameList((x + \" | Doctor\").split(\" | \")))\n",
    "match_df.loc[doctor_index, 'Name'] = match_df.loc[doctor_index].apply(lambda x: tNameList([ele.replace(\"Doctor\",\n",
    "                                                                                                       \"\").replace(\n",
    "    \"Docr\", \"\").replace('Docr', '').replace('Dortoe', '').replace('Dr ', '').replace(\"  \", \" \").strip() for ele in\n",
    "                                                                                           x['Name'].split(\" | \")]),\n",
    "                                                                      axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# captains/colonels\n",
    "officer_index = match_df[match_df['Name'].apply(lambda\n",
    "                                                    x: 'Col.' in x or 'Cols' in x or 'Colonel' in x or 'Coln' in x or 'Colo' in x or 'General' in x or 'Capt' in x or 'Captain' in x)].index\n",
    "match_df.loc[officer_index, 'Occupation'] = match_df.fillna(\"\").loc[officer_index, 'Occupation'].apply(\n",
    "    lambda x: tNameList((x + \" | Military Officer\").split(\" | \")))\n",
    "match_df.loc[officer_index, 'Name'] = match_df.loc[officer_index].apply(lambda x: tNameList([ele.replace(\n",
    "    \"Col.\", \"\").replace('Cols', '').replace('Colonel', '').replace('Coln', '').replace('Colo', '').replace('General',\n",
    "                                                                                                           '').replace(\n",
    "    'Captain', '').replace('Capt', '').replace(\"  \", \" \").strip() for ele in x['Name'].split(\" | \")]), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# next, we have a case where sometimes, one group name is matched to multiple people on ancestry, but we can actually reduce the number of people matched to by checking whether there are name(s) in the matched people from ancestry that correspond exactly to our \"Group Name\" column\n",
    "# if we can reduce the number of matches we do so, otherwise we keep the original matches\n",
    "mult_ind = df_final[df_final['Group Match Index'].apply(\n",
    "    lambda x: 'Unsearchable' not in x and x != '' and len(x.split(\" | \")) > 1)].index\n",
    "# getting match information\n",
    "df_final.loc[mult_ind, 'temp'] = df_final.loc[mult_ind].apply(\n",
    "    lambda x: [ind + \" | \" + match_df.loc[int(ind), 'Name'] for ind in x['Group Match Index'].split(\" | \")], axis=1)\n",
    "# filtering to see if we can reduce the number of matches\n",
    "df_final.loc[mult_ind, 'temp'] = df_final.loc[mult_ind].apply(\n",
    "    lambda x: [ele.split(\" | \")[0] for ele in x['temp'] if x['Group Name'] in ele], axis=1)\n",
    "# changing the match indices if there is a direct name corresopndence - otherwise we just keep the original match indices\n",
    "df_final.loc[mult_ind, 'Group Match Index'] = df_final.loc[mult_ind].apply(\n",
    "    lambda x: tNameList(x['temp']) if len(x['temp']) > 0 else x['Group Match Index'], axis=1)\n",
    "df_final.drop('temp', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# finally, we want to mark places where the ancestry.com match locations don't match up with what we have, and places where we imput location\n",
    "df_final['imputed_location'] = ''\n",
    "df_final['location conflict'] = ''\n",
    "ordering_dict = {'state': 0, 'county': 1, 'town': 2, 'village': 3, '' : -1}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def sameLocation(locations):\n",
    "    \"\"\"\n",
    "    This function takes in a list of locations and returns the location level that everything is the same at\n",
    "    :param locations: list of locations from the match data\n",
    "    :return: same location + location type\n",
    "    \"\"\"\n",
    "    states = list(set([loc[0] for loc in locations]))\n",
    "    counties = list(set([loc[1] for loc in locations]))\n",
    "    towns = list(set([loc[2] for loc in locations]))\n",
    "    villages = list(set([loc[3] if len(loc) > 3 else '' for loc in locations]))\n",
    "\n",
    "    loc = locations[0]\n",
    "    if len(villages) == 1 and '' not in villages:\n",
    "        return [loc, 'village']\n",
    "    elif len(towns) == 1 and '' not in towns:\n",
    "        return [[loc[0],loc[1],loc[2],''], 'town']\n",
    "    elif len(counties) == 1 and '' not in counties:\n",
    "        return [[loc[0],loc[1],'',''], 'county']\n",
    "    elif len(states) == 1 and '' not in states:\n",
    "        return [[loc[0],'',''], 'state']\n",
    "    else:\n",
    "        return ['No Match']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# add the location an individual has from ancestry.com if they only have one match\n",
    "temp_ind = df_final[df_final.apply(\n",
    "    lambda x: 'Unsearchable' not in x['Group Match Index'] and x['Group Match Index'] != '' and len(\n",
    "        x['Group Match Index'].split(\" | \")) == 1, axis=1)].index\n",
    "\n",
    "# we only add location if we think that the location ancestry.com has is more specific than what we have\n",
    "df_final.loc[temp_ind, 'temp'] = df_final.loc[temp_ind].apply(lambda x: match_df.loc[int(x['Group Match Index'])][\n",
    "    ['Match State', 'Match County', 'Match Town', 'Match Village']].values.tolist() if ordering_dict[\n",
    "                                                                                           x['Group Name Type']] <\n",
    "                                                                                       ordering_dict[match_df.loc[int(x[\n",
    "                                                                                                                          'Group Match Index']), 'Match Type']] else \"\",\n",
    "                                                              axis=1)\n",
    "\n",
    "# add location an individual has on ancestry.com if they have more than one match\n",
    "# pick indices\n",
    "mult_ind = df_final[df_final.apply(\n",
    "    lambda x: 'Unsearchable' not in x['Group Match Index'] and x['Group Match Index'] != '' and len(\n",
    "        x['Group Match Index'].split(\" | \")) > 1, axis=1)].index\n",
    "# find whether the multiple matches have the same location\n",
    "df_final.loc[mult_ind, 'temp'] = df_final.loc[mult_ind].apply(lambda x: sameLocation(match_df.loc[[int(ele) for ele in x['Group Match Index'].split(\" | \")], ['Match State', 'Match County', 'Match Town', 'Match Village']].values.tolist()), axis=1)\n",
    "# figure out the location type of the ancestry.com location\n",
    "df_final.loc[mult_ind, 'temp status'] = df_final.loc[mult_ind, 'temp'].apply(lambda x: x[1] if len(x)>1 else '')\n",
    "# remove from the location\n",
    "df_final.loc[mult_ind, 'temp'] = df_final.loc[mult_ind].apply(lambda x: x['temp'][0] if ordering_dict[x['temp status']]>ordering_dict[x['Group Name Type']] else '', axis = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# people for whom ancestry.com just messed up the matching - the state, county etc is wrong, so we want to remove these people from the match\n",
    "rem_ind = df_final[df_final.fillna(\"\")['temp'] != \"\"][df_final[df_final.fillna(\"\")['temp'] != \"\"].apply(\n",
    "    lambda x: statedict[x['Group State']] != x['temp'][0] and x['Group State'] != 'NY', axis=1)].index\n",
    "df_final.loc[rem_ind, 'Group Match Index'] = ''\n",
    "\n",
    "# places where the county that is given on ancestry is different from the county we have\n",
    "county_loc_conflict = df_final[df_final.fillna(\"\")['temp'] != \"\"][df_final[df_final.fillna(\"\")['temp'] != \"\"].apply(\n",
    "    lambda x: x['Group County'] != x['temp'][1] and x['Group County'] != '' and x['Group Match Index'] != '',\n",
    "    axis=1)].index\n",
    "df_final.loc[county_loc_conflict, 'location conflict'] = 'county'\n",
    "\n",
    "# places where the town that is given on ancestry is different from the tow  we have\n",
    "town_loc_conflict = df_final[df_final.fillna(\"\")['temp'] != \"\"][df_final[df_final.fillna(\"\")['temp'] != \"\"].apply(\n",
    "    lambda x: x['Group Town'] != x['temp'][2] and x['Group Town'] != '' and x['Group Match Index'] != '' and x[\n",
    "        'location conflict'] == '', axis=1)].index\n",
    "df_final.loc[town_loc_conflict, 'location conflict'] = 'town'\n",
    "\n",
    "# see if we can add location when we only have one match\n",
    "rep_ind = df_final[df_final.fillna(\"\")['temp'] != \"\"][df_final[df_final.fillna(\"\")['temp'] != \"\"].apply(\n",
    "    lambda x: x['location conflict'] == '' and x['Group Match Index'] != '', axis=1)].index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# places where the town that is given on ancestry is different from the town we have\n",
    "town_loc_conflict = df_final[df_final.fillna(\"\")['temp'] != \"\"][df_final[df_final.fillna(\"\")['temp'] != \"\"].apply(\n",
    "    lambda x: x['Group Town'] != x['temp'][2] and x['Group Town'] != '' and x['Group Match Index'] != '' and x[\n",
    "        'location conflict'] == '', axis=1)].index\n",
    "df_final.loc[town_loc_conflict, 'location conflict'] = 'town'\n",
    "\n",
    "# see if we can add location when we can recover the location\n",
    "rep_ind = df_final[df_final.fillna(\"\")['temp'] != \"\"][df_final[df_final.fillna(\"\")['temp'] != \"\"].apply(\n",
    "    lambda x: x['location conflict'] == '' and x['Group Match Index'] != '', axis=1)].index\n",
    "df_final.loc[rep_ind, 'imputed_location'] = df_final.loc[rep_ind].apply(\n",
    "    lambda x: match_df.loc[int(x['Group Match Index'])]['Match Type'] if pd.isnull(x['temp status']) else x[\n",
    "        'temp status'], axis=1)\n",
    "\n",
    "# add in imputed data\n",
    "df_final.loc[rep_ind, 'Group State'] = df_final.loc[rep_ind, 'temp'].apply(lambda x: statedict_rev[x[0]])\n",
    "df_final.loc[rep_ind, 'Group County'] = df_final.loc[rep_ind, 'temp'].apply(lambda x: x[1] + ' County')\n",
    "df_final.loc[rep_ind, 'Group Town'] = df_final.loc[rep_ind, 'temp'].apply(lambda x: x[2])\n",
    "df_final.loc[rep_ind, 'Group Village'] = df_final.loc[rep_ind, 'temp'].apply(lambda x: x[3])\n",
    "df_final.fillna(\"\", inplace=True)\n",
    "df_final.drop('temp', axis=1, inplace=True)\n",
    "df_final.drop('temp status', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# next, we're going to add the occupation information from ancestry.com into the occupation column in our main data\n",
    "match_df['Occupation'].fillna('', inplace=True)\n",
    "df_final['occupation'] = df_final.apply(lambda x: tNameList((\" | \".join(\n",
    "    [match_df.loc[int(ele), 'Occupation'] for ele in x['Group Match Index'].split(\" | \")]) + \" | \" + x[\n",
    "                                                                 'occupation']).split(\" | \")) if x[\n",
    "                                                                                                     'Group Match Index'] != '' and\n",
    "                                                                                                 x[\n",
    "                                                                                                     'Group Match Index'] != 'Unsearchable (not a name)' else\n",
    "x['occupation'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# import dictionary to clean occupations\n",
    "occ_data = pd.read_csv('clean_tools/occ_correction.csv')\n",
    "occ_dict = dict(zip(occ_data['Original'], occ_data['Corrected']))\n",
    "# manual additions to occupation dictionary because we can't work with commas in a csv file\n",
    "occ_dict[''] = ''\n",
    "occ_dict['Notary, Scrivenor & Broker'] = 'Broker'\n",
    "occ_dict['Notary, Scrivener & Broker'] = 'Broker'\n",
    "occ_dict['Notary, Scrivener, & Broker'] = 'Broker'\n",
    "# change occupations\n",
    "df_final['occupation'] = df_final['occupation'].apply(\n",
    "    lambda x: tNameList([str(occ_dict[ele]) for ele in x.split(\" | \")]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "match_df2 = match_df.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "match_df = match_df.loc[sorted(list(set([int(ele) for ele in df_final['Group Match Index'].apply(lambda x: x.split(\" | \") if x != '' and x != 'Unsearchable (not a name)' else []).explode().tolist() if not pd.isnull(ele)])))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# next, we want to eliminate cases in the match dataframe where for some reason, the location given in the match dataframe is too broad and we can eliminate some of them\n",
    "# we can do this because we now have more information about the location of the person, based off affiliated corporatiosn\n",
    "for ind in df_final[df_final['Group Match Index'].apply(lambda x: \"|\" in x)].index:\n",
    "    # get relevant data from our data and ancestry match data\n",
    "    match_data = match_df.loc[[int(ele) for ele in df_final.loc[ind, 'Group Match Index'].split(\" | \")]]\n",
    "    town, county = df_final.loc[ind, 'Group Town'], df_final.loc[ind, 'Group County']\n",
    "    match_town, match_county = [ele for ele in list(set(match_data['Match Town'].tolist())) if ele != \"\"], [ele for ele\n",
    "                                                                                                            in list(\n",
    "            set(match_data['Match County'].tolist())) if ele != \"\"]\n",
    "    # if there is more than one town, and one of the towns matches our data's town, we only keep that town\n",
    "    if len(match_town) > 1 and town in match_town:\n",
    "        match_ind = match_data[match_data['Match Town'] == town]['index_new'].tolist()\n",
    "        # print(ind, match_ind, tNameList([str(ele) for ele in match_ind]))\n",
    "        df_final.loc[ind, 'Group Match Index'] = tNameList([str(ele) for ele in match_ind])\n",
    "    # if there is more than one county, and one of the counties matches our data's county, we only keep that county\n",
    "    elif len(match_county) == 2 and county in match_county:\n",
    "        match_ind = match_data[match_data['Match County'] == county]['index_new'].tolist()\n",
    "        # print(ind, match_ind, tNameList([str(ele) for ele in match_ind]))\n",
    "        df_final.loc[ind, 'Group Match Index'] = tNameList([str(ele) for ele in match_ind])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5s/dvrxt95949x1pm_sjxm85lj00000gn/T/ipykernel_51442/706461425.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  match_list_no_dup.rename({'index_old': 'index_temp'}, axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# finally, we're going to create a new column in the match dataframe where all the indices of the people who are not matched to anyone are removed, so our match index goes from 0 to the total number of matches\n",
    "# first we remove the people who are not matched to anyone\n",
    "match_df = match_df.loc[sorted(list(set([int(ele) for ele in df_final['Group Match Index'].apply(lambda x: x.split(\" | \") if x != '' and x != 'Unsearchable (not a name)' else []).explode().tolist() if not pd.isnull(ele)])))]\n",
    "\n",
    "\n",
    "for ind in df_final[df_final['Group Match Index'].apply(lambda x: \"|\" in x)].index:\n",
    "    match_data = match_df.loc[[int(ele) for ele in df_final.loc[ind, 'Group Match Index'].split(\" | \")]]\n",
    "    town, county = df_final.loc[ind, 'Group Town'], df_final.loc[ind, 'Group County']\n",
    "    match_town, match_county = [ele for ele in list(set(match_data['Match Town'].tolist())) if ele != \"\"], [ele for ele\n",
    "                                                                                                            in list(\n",
    "            set(match_data['Match County'].tolist())) if ele != \"\"]\n",
    "    if len(match_town) == 2 and town in match_town:\n",
    "        match_ind = match_data[match_data['Match Town'] == town]['index_new'].tolist()\n",
    "        df_final.loc[ind, 'Group Match Index'] = tNameList([str(ele) for ele in match_ind])\n",
    "    elif len(match_county) == 2 and county in match_county:\n",
    "        match_ind = match_data[match_data['Match County'] == county]['index_new'].tolist()\n",
    "        df_final.loc[ind, 'Group Match Index'] = tNameList([str(ele) for ele in match_ind])\n",
    "\n",
    "match_df.drop(['index_temp', 'index_new'], inplace=True, axis=1)\n",
    "\n",
    "# next, we want to remove entries in match_list that are duplicated, and create a dictionary that maps the old indices in df_list to the new indices, after we drop duplicates in match_list\n",
    "\n",
    "# save old index\n",
    "match_df['index_old'] = match_df.index\n",
    "# drop duplicates, create temporary index column\n",
    "match_list_no_dup = match_df.drop_duplicates(subset=[ele for ele in match_df.columns if ele != 'index_old'])\n",
    "match_list_no_dup.rename({'index_old': 'index_temp'}, axis=1, inplace=True)\n",
    "\n",
    "# create mapping between old index, and temporary new index\n",
    "# the temporary new index removes indices of repeated values without renumbering anything\n",
    "match_dict_df = pd.merge(match_df.reset_index(),\n",
    "                         match_list_no_dup,\n",
    "                         how='left').set_index('index')\n",
    "match_dict_df['index_old'] = match_dict_df.index\n",
    "\n",
    "# now, we want to renumber the temporary index so that it is sequential and doesn't skip any numbers\n",
    "# we call this the new index\n",
    "gen_newind = match_dict_df[['index_temp']].drop_duplicates().reset_index(drop=True).copy()\n",
    "gen_newind['index_new'] = gen_newind.index\n",
    "# merge in new index to merged dataframe, map old index to new index\n",
    "match_dict_df = pd.merge(match_dict_df, gen_newind)\n",
    "match_dict = dict(zip(match_dict_df['index_old'], match_dict_df['index_new']))\n",
    "\n",
    "# change from old indices to new indices in df_list dataframe\n",
    "df_final['Group Match Index'] = df_final['Group Match Index'].apply(\n",
    "    lambda x: tNameList([str(match_dict[int(ele)]) for ele in x.split(' | ')]) if x not in [\"\",\n",
    "                                                                                            'Unsearchable (not a name)'] else \"\")\n",
    "# change match_list dataframe so that it removes duplicates and is indexed by the new index method\n",
    "match_df = pd.merge(match_list_no_dup, gen_newind)\n",
    "match_df['index_new'] = match_df['index_new'].apply(lambda x: str(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# find how many people own a particular asset\n",
    "# add that as suffix at end of debt asset\n",
    "asset_count_dict = df_final['assets'].apply(lambda x: [ele.split(\" : \")[0] for ele in x.split(\" | \")]).explode().value_counts().to_dict()\n",
    "asset_count_dict\n",
    "df_final['assets'] = df_final['assets'].apply(lambda x: \" | \".join([ele.split(\" : \")[0] + \"_\" + str(asset_count_dict[ele.split(\" : \")[0]]) + \" : \" + ele.split(\" : \")[1] for ele in x.split(\" | \")]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# total debt assets (not adjusted for ownership - sum is more than total amount of debt held\n",
    "df_final['6p_total'] = df_final['assets'].apply(lambda x: sum([pd.to_numeric(ele.split(\" : \")[1].split(\",\")[0]) for ele in x.split(\" | \")]))\n",
    "df_final['6p_def_total'] = df_final['assets'].apply(lambda x: sum([pd.to_numeric(ele.split(\" : \")[1].split(\",\")[1]) for ele in x.split(\" | \")]))\n",
    "df_final['unpaid_interest'] = df_final['assets'].apply(lambda x: sum([pd.to_numeric(ele.split(\" : \")[1].split(\",\")[2]) for ele in x.split(\" | \")]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# total debt assets (adjusted for ownership, assuming equal ownership)\n",
    "df_final['6p_total_adj'] = df_final['assets'].apply(lambda x: sum([pd.to_numeric(ele.split(\" : \")[1].split(\",\")[0])/pd.to_numeric(ele.split(\" : \")[0].split(\"_\")[2]) for ele in x.split(\" | \")]))\n",
    "df_final['6p_def_total_adj'] = df_final['assets'].apply(lambda x: sum([pd.to_numeric(ele.split(\" : \")[1].split(\",\")[1])/pd.to_numeric(ele.split(\" : \")[0].split(\"_\")[2]) for ele in x.split(\" | \")]))\n",
    "df_final['unpaid_interest_adj'] = df_final['assets'].apply(lambda x: sum([pd.to_numeric(ele.split(\" : \")[1].split(\",\")[2])/pd.to_numeric(ele.split(\" : \")[0].split(\"_\")[2]) for ele in x.split(\" | \")]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "df_final['final_total'] = df_final['6p_total'] + df_final['6p_def_total']\n",
    "df_final['final_total_adj'] = df_final['6p_total_adj'] + df_final['6p_def_total_adj']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "match_df.drop('index_temp', axis = 1).to_csv(\"../data_clean/match_data_CD.csv\")\n",
    "df_final.reset_index(drop = True).to_csv(\"../data_clean/final_data_CD.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}