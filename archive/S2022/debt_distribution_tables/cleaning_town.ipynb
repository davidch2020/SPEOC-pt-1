{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import process, fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Cleaning town names\n",
    "\n",
    "Goals:\n",
    "- Clean town names that are different but refer to the same town in `ASD_all.xlsx` and `CD_all.xlsx`.\n",
    "- check if all town names are in the \"town-county\" matching list given by `final_cw.xlsx` - if so, perform the matching.\n",
    "\n",
    "\n",
    "Step 1: flag by 1 records that have NA \"state\" and \"town\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Data/Post1790/Aggregated/raw/aggregated_CD.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/5s/dvrxt95949x1pm_sjxm85lj00000gn/T/ipykernel_58207/2830001104.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m     'ASD_all.xlsx', index_col=0).reset_index(drop=True).dropna(how='all').drop_duplicates().reset_index(drop=True)\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mCD_all\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../../Data/Post1790/Aggregated/raw/aggregated_CD.csv'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex_col\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;31m# strip white space\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    678\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    679\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 680\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    681\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    682\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    573\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    574\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 575\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    576\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    577\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    931\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    932\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[0;34m|\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 933\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    934\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    935\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1215\u001B[0m             \u001B[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1216\u001B[0m             \u001B[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1217\u001B[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001B[0m\u001B[1;32m   1218\u001B[0m                 \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1219\u001B[0m                 \u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    787\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m\"b\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    788\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 789\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    790\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    791\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../../Data/Post1790/Aggregated/raw/aggregated_CD.csv'"
     ]
    }
   ],
   "source": [
    "# import and cleaning\n",
    "ASD_all = pd.read_excel(\n",
    "    'ASD_all.xlsx', index_col=0).reset_index(drop=True).dropna(how='all').drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "CD_all = pd.read_csv('../../Data/Post1790/Aggregated/raw/aggregated_CD.csv', index_col=0).reset_index(drop=True)\n",
    "\n",
    "# strip white space\n",
    "for col in ['town', 'state', 'occupation']:\n",
    "    ASD_all[col] = ASD_all[col].str.strip()\n",
    "    CD_all[col] = CD_all[col].str.strip()\n",
    "\n",
    "ASD_all['orig_town'] = ASD_all['town']\n",
    "CD_all['orig_town'] = CD_all['town']\n",
    "    \n",
    "print(ASD_all.shape, CD_all.shape)\n",
    "\n",
    "CD_all.drop(['name_type', 'county'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ASD_all['FLAG'], CD_all['FLAG'] = 0, 0\n",
    "\n",
    "# identify NA rows\n",
    "ASD_all.loc[(pd.isna(ASD_all['town'])) | (pd.isna(ASD_all['state'])), 'FLAG'] = 1\n",
    "CD_all.loc[(pd.isna(CD_all['town'])) | (pd.isna(CD_all['state'])), 'FLAG'] = 1\n",
    "\n",
    "# select non-NA subdataframe\n",
    "ASD_no_NA = ASD_all[ASD_all['FLAG']==0]\n",
    "CD_no_NA = CD_all[CD_all['FLAG']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check all states\n",
    "print(\n",
    "    set(list(ASD_no_NA.state.unique()) + list(CD_no_NA.state.unique()))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Step 2: flag by 2 the records with states outside the following list of states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "state_list = ['RI', 'CT', 'GA', 'MD', 'NC', 'NH', 'NJ', 'NY', 'VA', 'PA', 'RI', 'SC', 'DE', 'MA', 'VT']\n",
    "state_name_list = [\n",
    "            'Rhode Island', 'Connecticut', 'Georgia', 'Maryland', 'North Carolina', \n",
    "            'New Hampshire', 'New Jersey', 'New York', 'Virginia',\n",
    "            'Pennsylvania', 'Rhode Island', 'South Carolina', 'Delaware', \n",
    "            'Massachusetts', 'Vermont'\n",
    "            ]\n",
    "state_list_dict = dict(zip(state_list, state_name_list))\n",
    "\n",
    "ASD_no_NA.loc[ASD_no_NA.apply(lambda row: row.state not in state_list, axis=1), 'FLAG'] = 2\n",
    "CD_no_NA.loc[CD_no_NA.apply(lambda row: row.state not in state_list, axis=1), 'FLAG'] = 2\n",
    "\n",
    "# update to the original df\n",
    "ASD_all.update(ASD_no_NA)\n",
    "CD_all.update(CD_no_NA)\n",
    "\n",
    "\n",
    "CD_all[CD_all['FLAG']==2].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Step 3: flag by 3 the records\n",
    "- with town name 'State of XX' or 'State XX' or 'XX State' where XX is the state where the town is located, and \n",
    "- with town name exactly or almost the same as the state name (due to typos).\n",
    "\n",
    "These are records for which \"township\" is not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ASD_rest = ASD_all[ASD_all['FLAG']==0]\n",
    "CD_rest = CD_all[CD_all['FLAG']==0]\n",
    "\n",
    "# state of XX/state XX - checked\n",
    "ASD_rest.loc[\n",
    "    ASD_rest.apply(lambda row: row.town.lower().startswith('state '), axis=1), 'FLAG'] = 3\n",
    "CD_rest.loc[\n",
    "    CD_rest.apply(lambda row: row.town.lower().startswith('state '), axis=1), 'FLAG'] = 3\n",
    "ASD_rest.loc[ASD_rest.town=='Delaware State', 'FLAG'] = 3\n",
    "CD_rest.loc[CD_rest.town=='Delaware State', 'FLAG'] = 3\n",
    "\n",
    "# town name == state name - checked\n",
    "ASD_rest.loc[ASD_rest.apply(lambda row: row.town==state_list_dict[row.state], axis=1), 'FLAG'] = 3\n",
    "CD_rest.loc[CD_rest.apply(lambda row: row.town==state_list_dict[row.state], axis=1), 'FLAG'] = 3\n",
    "\n",
    "# town name ~= state name - checked\n",
    "ASD_rest.loc[\n",
    "    ASD_rest.apply(lambda row: \n",
    "    process.cdist([row.town], [state_list_dict[row.state]])[0][0] >= 80, axis=1), 'FLAG'\n",
    "    ] = 3\n",
    "CD_rest.loc[\n",
    "    CD_rest.apply(lambda row: \n",
    "    process.cdist([row.town], [state_list_dict[row.state]])[0][0] >= 80, axis=1), 'FLAG'\n",
    "    ] = 3\n",
    "\n",
    "# Carolina in South Carolina (no county named Carolina)\n",
    "ASD_rest.loc[\n",
    "    ASD_rest.apply(lambda row: row.town in state_list_dict[row.state] and\n",
    "    row.town != state_list_dict[row.state], axis=1), 'FLAG'] = 3\n",
    "CD_rest.loc[\n",
    "    CD_rest.apply(lambda row: row.town in state_list_dict[row.state] and\n",
    "    row.town != state_list_dict[row.state], axis=1), 'FLAG'] = 3\n",
    "\n",
    "# update\n",
    "ASD_all.update(ASD_rest)\n",
    "CD_all.update(CD_rest)\n",
    "CD_all[CD_all['FLAG']==3].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Step 4: unify town names\n",
    "- remove `\"state of\" + state_name` and `\"of\" + state_name` that appear in the town name, except \"City of New York\",\n",
    "- remove `\"state\" + state_name` that appears in the town name,\n",
    "- remove `state_name` from `XX + state_name` or `state_name + XX`,\n",
    "- take into account the three special cases.\n",
    "\n",
    "Then, \n",
    "- create a new identifier and remove \"Town\"/\"County\" from town names.\n",
    "\n",
    "Flag this change by 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ASD_rest = ASD_all[ASD_all.FLAG==0]\n",
    "CD_rest = CD_all[CD_all.FLAG==0]\n",
    "\n",
    "def remove_state_from_town(row):\n",
    "    state_name = state_list_dict[row.state]\n",
    "    # state of/of\n",
    "    row.town = row.town.replace(' State of ' + state_name, '')\n",
    "    # state + state_name\n",
    "    row.town = row.town.replace(' State ' + state_name, '')\n",
    "    # special cases\n",
    "    if row.town == 'Boston state Massachusetts':\n",
    "        row.town = 'Boston'\n",
    "\n",
    "    if row.town == 'New Castle County Delaware State':\n",
    "        row.town = 'New Castle County'\n",
    "\n",
    "    if row.town == 'Virginia and Philadelphia':\n",
    "        row.town = 'Philadelphia'\n",
    "\n",
    "    if row.town != 'City of New York':\n",
    "        row.town = row.town.replace(' of ' + state_name, '')\n",
    "        row.town = row.town.replace(state_name, '')\n",
    "    \n",
    "    # flag changes\n",
    "    row.FLAG = 4\n",
    "    return row\n",
    "\n",
    "ASD_rest = ASD_rest.apply(lambda row: remove_state_from_town(row), axis=1)\n",
    "CD_rest = CD_rest.apply(lambda row: remove_state_from_town(row), axis=1)\n",
    "\n",
    "# town_level = 'T' if specified 'Town', 'C' if specified 'County', otherwise 'U'\n",
    "ASD_rest['town_level'] = 'U'\n",
    "ASD_rest.loc[ASD_rest['town'].str.contains('County', na=False), 'town_level'] = 'C'\n",
    "ASD_rest.loc[ASD_rest['town'].str.contains('Town', na=False), 'town_level'] = 'T'\n",
    "\n",
    "def remove_CountyTown(row):\n",
    "    row.town = row.town.replace('County', '')\n",
    "    row.town = row.town.replace('Town', '')\n",
    "    row.FLAG = 4\n",
    "    return row\n",
    "    \n",
    "ASD_rest = ASD_rest.apply(lambda row: remove_CountyTown(row), axis=1)\n",
    "CD_rest = CD_rest.apply(lambda row: remove_CountyTown(row), axis=1)\n",
    "\n",
    "\n",
    "ASD_all.update(ASD_rest)\n",
    "CD_all.update(CD_rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Step 5: match town names that are likely to be the same one  \n",
    "1. for each state, create a list `A` of town names with # of occurences >= 4; create a list `B` for all the rest towns.\n",
    "2. for each town in list `B`, find the best three matches with towns in list `A` - if above a threshold, report all three and match to the best one. - to be checked afterwards\n",
    "3. all unmatched towns in list `B` become a new list `C` - we WANT to compare one another and if the similarity is above a threshold, group them; otherwise keep it untouched. One simple procedure is for each town in list `C`, group it with all other towns whose distance to it is smaller than a threshold. Then proceed to the next one if it's not in some group already and skip otherwise. This is legitimate because we expect \"typos\" to cause small differences among all mistyped names. Report all the matched and unmatched cases in this round.\n",
    "4. For all above, allow user's input to manually confirm the matches.\n",
    "\n",
    "Flag this change by 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def match_towns(df):\n",
    "    if len(df.index)==0:    # Vermont\n",
    "        return df\n",
    "    print(f\"State: {df.state.iloc[0]}\")\n",
    "\n",
    "    val_counts = df.town.value_counts(sort=True)\n",
    "    list_A, list_B = list(val_counts[val_counts>=min(val_counts.iloc[0], 4)].index), list(val_counts[val_counts<min(val_counts.iloc[0], 6)].index)\n",
    "    list_C = list_B.copy()\n",
    "    for town in list_B:\n",
    "        best3 = process.extract(town, list_A)[0:3]\n",
    "        if best3[0][1] >= 85:\n",
    "            print(f\"{town}. Candidates: {[x[0] for x in best3]}. Matched: {best3[0][0]}.\\n\")\n",
    "\n",
    "            # # user input\n",
    "            # ACCEPT = input(\"1 for ACCEPT. 0 for REJECT\")\n",
    "            # # records\n",
    "\n",
    "            # if ACCEPT==1:\n",
    "            # matched to list_A\n",
    "            list_C.remove(town)\n",
    "            df.loc[df.town==town, 'FLAG'] = 5\n",
    "            df.loc[df.town==town, 'town'] = best3[0][0]\n",
    "\n",
    "    list_C_flag = [-1 for x in list_C]\n",
    "    for id, town in enumerate(list_C):\n",
    "        # only do matching if not already matched\n",
    "        if list_C_flag[id] == -1:\n",
    "            bests = [x[0] for x in process.extract(town, list_C, score_cutoff=85)]\n",
    "            if len(bests) > 1:\n",
    "                # if not just oneself\n",
    "                indexes = [list_C.index(x) for x in bests]\n",
    "                # make sure same group has the same id\n",
    "                print(f\"Candidate group: {bests} -> {min(bests, key=len)}\")\n",
    "\n",
    "                # pick the shortest one as the name we want to keep\n",
    "                # this deals with the cases like North Hampton -> Hampton\n",
    "                index_selected = list_C.index(min(bests, key=len))\n",
    "                \n",
    "                for k in indexes: \n",
    "                    list_C_flag[k] = index_selected \n",
    "\n",
    "                # one special case\n",
    "                if bests==['Cumberland', 'Cumb  ']:\n",
    "                    list_C_flag[k] = list_C.index('Cumberland')\n",
    "    \n",
    "    for id, flag in enumerate(list_C_flag):\n",
    "        if flag != -1:\n",
    "            df.loc[df.town==list_C[id], 'FLAG'] = 5\n",
    "            df.loc[df.town==list_C[id], 'town'] = list_C[flag]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# first stripping trailing space\n",
    "ASD_rest['town'] = ASD_rest['town'].apply(lambda x: x.strip())\n",
    "CD_rest['town'] = CD_rest['town'].apply(lambda x: x.strip())\n",
    "\n",
    "# first match to show list (but no update)\n",
    "for state_code in state_list:\n",
    "    \n",
    "    ASD_rest[ASD_rest.state==state_code] = match_towns(ASD_rest[ASD_rest.state==state_code])\n",
    "    CD_rest[CD_rest.state==state_code] = match_towns(CD_rest[CD_rest.state==state_code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# keep misclassified cases untouched\n",
    "ASD_rest['mskip'], CD_rest['mskip'] = 0, 0\n",
    "\n",
    "ASD_rest.loc[\n",
    "    ASD_rest.apply(lambda row: row.town in ['Stafford', 'New Hartford', 'New Brunswick', 'St Lukes', \"St George's Parish\"], axis=1), 'mskip'\n",
    "] = 1\n",
    "CD_rest.loc[\n",
    "    CD_rest.apply(lambda row: row.town in ['Stafford', 'New Hartford', 'New Brunswick', 'St Lukes', \"St George's Parish\"], axis=1), 'mskip'\n",
    "] = 1\n",
    "\n",
    "ASD_rest.loc[ASD_rest.town=='George town', 'town'] = 'Georgetown'\n",
    "CD_rest.loc[CD_rest.town=='George town', 'town'] = 'Georgetown'\n",
    "\n",
    "# keep misgrouped cases untouched\n",
    "ASD_rest.loc[ASD_rest.town==\"St Johnn's Parish\", 'town'] = \"St John's Parish\"\n",
    "CD_rest.loc[CD_rest.town==\"St Johnn's Parish\", 'town'] = \"St John's Parish\"\n",
    "\n",
    "ASD_rest.loc[ASD_rest.town==\"St Bartholomew's\", 'town'] = \"St Bartholomew's Parish\"\n",
    "CD_rest.loc[CD_rest.town==\"St Bartholomew's\", 'town'] = \"St Bartholomew's Parish\"\n",
    "ASD_rest.loc[ASD_rest.town==\"Bartholomew's Parish\", 'town'] = \"St Bartholomew's Parish\"\n",
    "CD_rest.loc[CD_rest.town==\"Bartholomew's Parish\", 'town'] = \"St Bartholomew's Parish\"\n",
    "\n",
    "ASD_rest.loc[\n",
    "    ASD_rest.apply(\n",
    "        lambda row: row.town in [\"St Luke's\", \"St John's Parish\", \"St Bartholomew's\",  'St Helena', 'St Gustavus', \"John's Island\"], axis=1\n",
    "        ), 'mskip'\n",
    "] = 1\n",
    "CD_rest.loc[\n",
    "    CD_rest.apply(\n",
    "        lambda row: row.town in [\"St Luke's\", \"St John's Parish\", \"St Bartholomew's\",  'St Helena', 'St Gustavus', \"John's Island\"], axis=1\n",
    "        ), 'mskip'\n",
    "] = 1\n",
    "\n",
    "ASD_rest.loc[ASD_rest.apply(lambda row: row.town in ['Richard Sennings', 'Richard'], axis=1), 'mskip'] = 1\n",
    "CD_rest.loc[CD_rest.apply(lambda row: row.town in ['Richard Sennings', 'Richard'], axis=1), 'mskip'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# match again\n",
    "ASD_rest = ASD_rest[ASD_rest.mskip==0]\n",
    "CD_rest = CD_rest[CD_rest.mskip==0]\n",
    "\n",
    "for state_code in state_list:\n",
    "    \n",
    "    ASD_rest[ASD_rest.state==state_code] = match_towns(ASD_rest[ASD_rest.state==state_code])\n",
    "    CD_rest[CD_rest.state==state_code] = match_towns(CD_rest[CD_rest.state==state_code])\n",
    "\n",
    "# deal with some other special cases\n",
    "ASD_rest.loc[ASD_rest.town=='East Haddam', 'town'] = 'Haddam'\n",
    "CD_rest.loc[CD_rest.town=='East Haddam', 'town'] = 'Haddam'\n",
    "\n",
    "ASD_all.update(ASD_rest)\n",
    "CD_all.update(CD_rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Now maps to the County-town list\n",
    "\n",
    "There are a few cases where the `orig_town` names include `'County'`, but gets mapped to nothing on the list. We assign `county` to `name_type` in this case, and let `county` be county name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ASD_no_NA = ASD_all[ASD_all['FLAG']!=1]\n",
    "CD_no_NA = CD_all[CD_all['FLAG']!=1]\n",
    "\n",
    "matchlist = pd.read_csv('../../Data/AssetGeography/county_cw.csv', index_col=0)\n",
    "matchlist = matchlist.rename({'town': 'orig_town'}, axis=1)\n",
    "matchlist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ASD_merged = pd.merge(left=ASD_no_NA, right=matchlist, how='left', on=['state', 'orig_town'])\n",
    "CD_merged = pd.merge(left=CD_no_NA, right=matchlist, how='left', on=['state', 'orig_town'])\n",
    "\n",
    "# select\n",
    "filter_ASD = (ASD_merged.orig_town.str.contains('County')) & (ASD_merged.name_type.isna())\n",
    "filter_CD = (CD_merged.orig_town.str.contains('County')) & (CD_merged.name_type.isna())\n",
    "ASD_merged.loc[filter_ASD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ASD_merged.loc[filter_ASD, 'name_type'] = 'county'\n",
    "ASD_merged.loc[filter_ASD, 'county'] = ASD_merged.loc[filter_ASD, 'town']\n",
    "\n",
    "CD_merged.loc[filter_CD, 'name_type'] = 'county'\n",
    "CD_merged.loc[filter_CD, 'county'] = CD_merged.loc[filter_CD, 'town']\n",
    "\n",
    "CD_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# address Chris's comments\n",
    "ASD_merged.loc[ASD_merged.town=='Brunswick', 'town'] = 'New Brunswick'\n",
    "CD_merged.loc[CD_merged.town=='Brunswick', 'town'] = 'New Brunswick'\n",
    "\n",
    "ASD_merged.loc[(ASD_merged.town=='Johnson') & (ASD_merged.state=='RI'), 'town'] = 'Johnston'\n",
    "CD_merged.loc[(CD_merged.town=='Johnson') & (CD_merged.state=='RI'), 'town'] = 'Johnston'\n",
    "ASD_merged.loc[(ASD_merged.town=='Georges') & (ASD_merged.state=='RI'), 'town'] = 'Georgetown'\n",
    "CD_merged.loc[(CD_merged.town=='Georges') & (CD_merged.state=='RI'), 'town'] = 'Georgetown'\n",
    "\n",
    "# Chester\n",
    "ASD_merged.loc[(ASD_merged.orig_town.str.contains('Chester and County')) |\n",
    "            (ASD_merged.orig_town.str.contains('Chester County'))  |\n",
    "            (ASD_merged.orig_town.str.contains('Chester Co')), 'name_type'] = 'county'\n",
    "\n",
    "CD_merged.loc[(CD_merged.orig_town.str.contains('Chester and County')) |\n",
    "            (CD_merged.orig_town.str.contains('Chester County'))  |\n",
    "            (CD_merged.orig_town.str.contains('Chester Co')), 'name_type'] = 'county'\n",
    "    \n",
    "# Northern Liberties - but already marked as county\n",
    "ASD_merged.loc[ASD_merged.town=='Northern Liberties', 'name_type'] = 'county'\n",
    "CD_merged.loc[CD_merged.town=='Northern Liberties', 'name_type'] = 'county'\n",
    "\n",
    "ASD_merged.loc[(ASD_merged.town=='Cumb') | (ASD_merged.town=='Cumberland'), 'name_type'] = 'county'\n",
    "CD_merged.loc[(CD_merged.town=='Cumb') | (CD_merged.town=='Cumberland'), 'name_type'] = 'county'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# assign nan to town if name_type == county/state\n",
    "ASD_merged.loc[(ASD_merged.name_type=='county') | (ASD_merged.name_type=='state'), 'town'] = np.nan\n",
    "CD_merged.loc[(CD_merged.name_type=='county') | (CD_merged.name_type=='state'), 'town'] = np.nan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# manual fixes - Chris\n",
    "CD_merged.loc[CD_merged.query('orig_town==\"Northan Liberties\"').index, ['county', 'name_type']] = ['Philadelphia County', 'town']\n",
    "CD_merged.loc[CD_merged.query('orig_town==\"Northern Liberties\"').index, ['county', 'name_type']] = ['Philadelphia County', 'town']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CD_merged.loc[CD_merged.query('town == \"Doden\" and name_type == \"other\"').index, ['town', 'name_type']] = [np.nan, 'state_flag']\n",
    "CD_merged.loc[CD_merged.query('town == \"Huntington\" and name_type == \"other\"').index, ['town', 'county', 'name_type']] = [np.nan, 'Hunterdon County', 'county']\n",
    "CD_merged.loc[CD_merged.query('town == \"Northumberland\" and name_type == \"other\"').index, ['town', 'county', 'name_type']] = ['Northumberland', 'Northumberland County', 'town']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CD_merged.loc[CD_merged.query('town == \"City of New York\" and name_type == \"other\"').index, ['town', 'county', 'name_type']] = ['New York City', 'New York County', 'town']\n",
    "CD_merged.loc[CD_merged[CD_merged['town'].apply(lambda x: \"State\" in x if not pd.isnull(x) else False)].index, ['town', 'name_type']] = [np.nan, 'state']\n",
    "CD_merged.loc[CD_merged.query('town == \"Vermont\"').index, ['town', 'state', 'name_type']] = [np.nan, 'VT', 'state']\n",
    "CD_merged.loc[CD_merged.query('town == \"North Hampshire\" and name_type == \"other\"').index, ['town', 'name_type']] = [np.nan, 'state_flag']\n",
    "CD_merged.loc[CD_merged.query('town == \"Long Island\" and name_type == \"other\"').index, ['town', 'name_type']] = [np.nan, 'state']\n",
    "CD_merged.loc[CD_merged.query('town == \"Carolina\" and name_type == \"other\"').index, ['town', 'name_type']] = [np.nan, 'state_flag']\n",
    "CD_merged.loc[CD_merged.query('town == \"Isaac\" and name_type == \"other\"').index, ['town', 'name_type']] = [np.nan, 'state_flag']\n",
    "CD_merged.loc[CD_merged.query('town == \"Kittery\" and name_type == \"other\"').index, ['town', 'name_type']] = [np.nan, 'state_flag']\n",
    "CD_merged.loc[CD_merged.query('town == \"Glouster\" and name_type == \"other\"').index, ['town', 'name_type']] = [np.nan, 'state_flag']\n",
    "CD_merged.loc[CD_merged.query('town == \"York\" and name_type == \"other\"').index, ['town', 'name_type']] = [np.nan, 'state_flag']\n",
    "CD_merged.loc[CD_merged.query('town == \"Wells\" and name_type == \"other\"').index, ['town', 'name_type']] = [np.nan, 'state_flag']\n",
    "CD_merged.loc[CD_merged.query('town == \"W Callisters\" and name_type == \"other\"').index, ['town', 'name_type']] = [np.nan, 'state_flag']\n",
    "CD_merged.loc[CD_merged.query('town == \"Connecticutt\" and name_type == \"other\"').index, ['town', 'name_type']] = [np.nan,  'county']\n",
    "CD_merged.loc[CD_merged.query('town == \"Charleston South Carolina\" and name_type == \"other\"').index, ['town', 'county', 'name_type']] = ['Charleston', 'Charleston County', 'town']\n",
    "CD_merged.loc[CD_merged.query('town == \"Albany\" and name_type == \"other\"').index, ['town', 'county', 'name_type']] = ['Charleston', 'Charleston County', 'town']\n",
    "CD_merged.loc[CD_merged.query('town == \"Springfield\" and name_type == \"other\"').index, ['town', 'name_type']] = [np.nan, 'state_flag']\n",
    "CD_merged.loc[CD_merged.query('town == \"Long Cames\" and name_type == \"other\"').index, ['town', 'name_type']] = [np.nan, 'state_flag']\n",
    "CD_merged.loc[CD_merged.query('town == \"Savannah\" and name_type == \"other\"').index, ['town', 'name_type']] = [np.nan, 'state_flag']\n",
    "CD_merged[CD_merged['name_type'] == 'other'][['town', 'state']].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ASD_df= ASD_merged.drop(['orig_town'], axis=1)\n",
    "CD_df = CD_merged.drop(['orig_town'], axis=1)\n",
    "\n",
    "ASD_df_strNaN = ASD_df.fillna('NaN')\n",
    "CD_df_strNaN = CD_df.fillna('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "states = {\"Connecticut\": \"CT\",\"Delaware\": \"DE\",\"Georgia\": \"GA\", \"Maryland\": \"MD\", \"Massachusetts\": \"MA\",\n",
    "          \"New Hampshire\": \"NH\", \"New Jersey\": \"NJ\", \"New York\": \"NY\",\"North Carolina\": \"NC\",\n",
    "          \"Pennsylvania\": \"PA\", \"Rhode Island\": \"RI\", \"South Carolina\": \"SC\", \"Virginia\": \"VA\", }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# aggregate data from colonies & noncolonies\n",
    "CD_df_str_names = CD_df.copy()\n",
    "CD_df_str_names[['county', 'town']] = CD_df_str_names[['county', 'town']].fillna('nan')\n",
    "colonies = CD_df_str_names.loc[(~CD_merged.name_type.isna()) & (CD_merged.name_type!='other')]\n",
    "noncolonies = CD_df_str_names.loc[(CD_merged.name_type.isna()) | (CD_merged.name_type=='other')]\n",
    "colonies = pd.concat([colonies, noncolonies[noncolonies['state'].apply(lambda x: x in states.values())]])\n",
    "noncolonies = noncolonies[noncolonies['state'].apply(lambda x: x not in states.values())]\n",
    "\n",
    "# find total amount of assets held in each town-county-state\n",
    "aggregated_data = colonies.groupby(['state', 'county', 'town'], dropna=False).agg({'6p_total':['sum', 'size']}).reset_index()\n",
    "aggregated_data.columns = ['state', 'county', 'town', '6p_total_sum', '6p_total_count']\n",
    "aggregated_data['town/county pct'] = np.round(aggregated_data['6p_total_sum'] / \\\n",
    "                                     aggregated_data.groupby(['state', 'county'])['6p_total_sum'].transform('sum')*100, 1)\n",
    "aggregated_data['county/state pct'] = np.round(aggregated_data.groupby(['state', 'county'])['6p_total_sum'].transform('sum') / \\\n",
    "                                    aggregated_data.groupby('state')['6p_total_sum'].transform('sum')*100, 1)\n",
    "aggregated_data['state/ovall (excluding non-colonies) pct'] = np.round(aggregated_data.groupby(['state'])['6p_total_sum'].transform('sum') / \\\n",
    "                                                              aggregated_data['6p_total_sum'].sum()*100, 1)\n",
    "\n",
    "noncolonies_agg = noncolonies.groupby(['state', 'county', 'town'], dropna=False).agg({'6p_total':['sum', 'size']}).reset_index()\n",
    "noncolonies_agg.columns = ['state', 'county', 'town', '6p_total_sum', '6p_total_count']\n",
    "#pd.concat([aggregated_data, noncolonies_agg]).to_csv('../Results/DebtDistribution/CD_geographical_table_summary.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# export crosswalk for original town name, correct town name, county and state\n",
    "mapping_df = CD_merged[['orig_town', 'town', 'county', 'state', 'name_type']].drop_duplicates()\n",
    "mapping_df.drop_duplicates().to_csv('../../Data/AssetGeography/final_geographical_cw.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final Cleaning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import CD data\n",
    "CD_all = pd.read_csv(\"../../Data/Post1790/Aggregated/raw/aggregated_CD.csv\", index_col = 0)\n",
    "cw = pd.read_csv(\"../../Data/AssetGeography/final_geographical_cw.csv\", index_col = 0)\n",
    "# reformat names\n",
    "CD_all['Name'] = CD_all['Name'].apply(lambda x: x.replace('\\'','').replace('\\\"','').strip('][').split(', '))\n",
    "# add \"right\" town name to data\n",
    "CD_all.rename({'town':'orig_town'}, axis = 1, inplace = True)\n",
    "CD_all[['orig_town', 'county', 'state', 'name_type']] = CD_all[['orig_town', 'county', 'state', 'name_type']].fillna('nan')\n",
    "cw[['orig_town', 'county', 'state', 'name_type']] = cw[['orig_town', 'county', 'state', 'name_type']].fillna('nan')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CD_merged = pd.merge(CD_all, cw, on = ['orig_town', 'county', 'state'], how = 'left')\n",
    "CD_merged = CD_merged.replace('nan', np.nan)\n",
    "CD_merged.loc[CD_merged.query('orig_town.isna() and county.isna() and not state.isna() and state != \"FR\"').index, 'name_type_x'] = 'state'\n",
    "CD_merged['name_type'] = [x if pd.isnull(y) else y for x, y in zip(CD_merged['name_type_x'], CD_merged['name_type_y'])]\n",
    "CD_merged.drop(['name_type_x', 'name_type_y'], axis = 1, inplace = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Manually fixing CD_merged\n",
    "CD_merged.loc[CD_merged.query('orig_town == \"Colchester\"').index, ['state', 'county', 'name_type', 'town']] = ['CT', 'New London County', 'town', 'Colchester']\n",
    "CD_merged.loc[CD_merged.query('orig_town == \"Cumberland\" and state == \"PA\"').index, 'name_type'] = 'county'\n",
    "CD_merged.loc[CD_merged.query('orig_town == \"Cumberland\" and state == \"RI\"').index, 'town'] = 'Cumberland'\n",
    "CD_merged.loc[CD_merged.query('orig_town == \"Northan Liberties\" and state == \"PA\"').index, 'town'] = 'Philadelphia'\n",
    "CD_merged.loc[CD_merged.query('orig_town == \"Northern Liberties\" and state == \"PA\"').index, 'town'] = 'Philadelphia'\n",
    "CD_merged.loc[CD_merged.query('orig_town == \"Newark\"').index, ['town', 'state', 'county', 'name_type']] = ['Newark', 'NJ', 'Newark County', 'town']\n",
    "CD_merged.loc[CD_merged.query('orig_town == \"Philadelphia\" and town.isna()').index, ['state', 'county', 'name_type', 'town']] = ['PA', 'Philadelphia County',\n",
    "                                                                                                                                'town', 'Philadelphia']\n",
    "CD_merged.loc[CD_merged.query('orig_town == \"Halifax \"').index, 'town'] = 'Halifax'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CD_merged['county'] = CD_merged['county'].apply(lambda x: x.replace(\"Prince Georges County\", \"Prince George\\'s County\") if not pd.isnull(x) else x)\n",
    "CD_merged['county'] = CD_merged['county'].apply(lambda x: x.replace(\"St Marys County\", \"St Mary\\'s County\")  if not pd.isnull(x) else x)\n",
    "CD_merged['county'] = CD_merged['county'].apply(lambda x: x.replace(\"Queen Annes County\", \"Queen Anne\\'s County\")  if not pd.isnull(x) else x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CD_merged['Name'] = CD_merged['Name'].apply(lambda x: [] if (x == [' '] or x == [' ']) else x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CD_merged.loc[CD_merged[CD_merged['Name'].apply(lambda x: 'Jacob Myers' in x)].index, 'town'] = 'Burlington'\n",
    "CD_merged.loc[CD_merged[CD_merged['Name'].apply(lambda x: 'John Otto' in x)].index, 'town'] = 'Reading'\n",
    "CD_merged.loc[CD_merged[CD_merged['Name'].apply(lambda x: 'Manuel Eyre' in x)].index, ['town', 'county', 'name_type']] = ['Kensington','Westmoreland County', 'town']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CD_merged.loc[CD_merged.query('name_type == \"county\" and county.isna()').index, 'name_type'] = 'state'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CD_merged.to_csv(\"../../Data/Post1790/Aggregated/raw/aggregated_CD_final.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CD_merged[CD_merged['orig_town'].apply(lambda x: 'mansfield' in x.lower())]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}